{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08aacaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython\n",
      "Set autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Motif Conservation Analysis Pipeline\n",
    "\n",
    "1. Pick a latent you like to do an initial experiment on (eventually do all)\n",
    "2. Find the top 100 activations across all proteins. So even if there's multiple highly activated residues in one protein, take all of them. Note down the position of each of the top activations and note down which protein\n",
    "3. For each top activating residue, get the neighborhood of residues (say +- 10 just for example)\n",
    "4. It is already aligned because the central residue in each neighborhood is a max activated residues\n",
    "5. Make the sequence logo as per the wiki link I sent, with the central residue of the logo being the residue that \"seeded\" the neighborhood due to its being in the top-100 most highly activated residues\n",
    "\n",
    "Important Notes:\n",
    "- Protein language models add special tokens (BOS/EOS), so token positions != residue positions\n",
    "- Token 0: <cls>/<bos>, Tokens 1 to N: amino acids, Token N+1: <eos>\n",
    "- We properly map token indices to residue indices and handle padding for incomplete neighborhoods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../plm_circuits')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from Bio import SeqIO\n",
    "import pathlib\n",
    "import heapq\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import logomaker\n",
    "import matplotlib.pyplot as plt\n",
    "# Import utility functions\n",
    "from helpers.utils import load_esm, load_sae_prot, cleanup_cuda\n",
    "from hook_manager import SAEHookProt\n",
    "\n",
    "from enrich_act import load_cached_activations, EfficientActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11dbff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All helpers here \n",
    "\n",
    "# Define a named tuple to store activation information\n",
    "ActivationInfo = namedtuple('ActivationInfo', ['activation_value', 'protein_idx', 'token_idx', 'protein_id', 'residue_idx'])\n",
    "\n",
    "\n",
    "def find_top_k_activations_streamlined(cache_system, layer_idx, latent_idx, k=100):\n",
    "    \"\"\"\n",
    "    Streamlined approach: single pass, one max per protein, top K globally.\n",
    "    \"\"\"\n",
    "    print(f\"Finding top {k} activations for layer {layer_idx}, latent {latent_idx}...\")\n",
    "    print(\"Using streamlined single-pass approach (one max per protein)\")\n",
    "    \n",
    "    # Use a min-heap to efficiently maintain top-K\n",
    "    top_k_heap = []\n",
    "    \n",
    "    for protein_idx in tqdm(range(len(cache_system.metadata['proteins'])), desc=\"Processing proteins\"):\n",
    "        try:\n",
    "            protein_data = cache_system.metadata['proteins'][protein_idx]\n",
    "            protein_id = protein_data['protein_id']\n",
    "            sequence_length = protein_data['sequence_length']\n",
    "            \n",
    "            # Load the specific layer file\n",
    "            layer_file = protein_data[f'layer_{layer_idx}_file']\n",
    "            \n",
    "            if cache_system.use_sparse:\n",
    "                sparse_matrix = load_npz(layer_file)\n",
    "                latent_column = sparse_matrix[:, latent_idx].toarray().flatten()\n",
    "            else:\n",
    "                acts_np = np.load(layer_file)\n",
    "                latent_column = acts_np[:, latent_idx]\n",
    "            \n",
    "            # # Find max activation and its position\n",
    "            # if len(latent_column) > 0:\n",
    "            #     valid_length = min(len(latent_column), sequence_length)\n",
    "            #     valid_activations = latent_column[:valid_length]\n",
    "                \n",
    "            #     if len(valid_activations) > 0:\n",
    "            max_idx = np.argmax(latent_column)\n",
    "            max_activation = float(latent_column[max_idx])\n",
    "            \n",
    "            activation_info = ActivationInfo(\n",
    "                activation_value=max_activation,\n",
    "                protein_idx=protein_idx,\n",
    "                token_idx=max_idx + 1,  # +1 for BOS token offset\n",
    "                protein_id=protein_id,\n",
    "                residue_idx=max_idx  # Direct residue position\n",
    "            )\n",
    "            \n",
    "            # Maintain top-K using heap\n",
    "            if len(top_k_heap) < k:\n",
    "                heapq.heappush(top_k_heap, (-max_activation, protein_idx, activation_info))\n",
    "            elif max_activation > -top_k_heap[0][0]:\n",
    "                heapq.heapreplace(top_k_heap, (-max_activation, protein_idx, activation_info))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein {protein_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Extract results and sort by activation value (highest first)\n",
    "    top_k_activations = [item[2] for item in top_k_heap]\n",
    "    top_k_activations.sort(key=lambda x: x.activation_value, reverse=True)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"- Processed {len(cache_system.metadata['proteins'])} proteins\")\n",
    "    print(f\"- Found {len(top_k_activations)} top activations\")\n",
    "    if top_k_activations:\n",
    "        print(f\"- Activation range: {top_k_activations[0].activation_value:.4f} to {top_k_activations[-1].activation_value:.4f}\")\n",
    "    \n",
    "    return top_k_activations\n",
    "\n",
    "def extract_neighborhoods(top_activations, protein_sequences, window_size=10, pad_token='X'):\n",
    "    \"\"\"\n",
    "    Extract amino acid neighborhoods around top activating positions.\n",
    "    Handles proper token-to-residue mapping and pads incomplete neighborhoods.\n",
    "    \n",
    "    Args:\n",
    "        top_activations: List of ActivationInfo tuples\n",
    "        protein_sequences: Dict mapping protein_id to sequence string\n",
    "        window_size: Number of residues on each side of the central residue\n",
    "        pad_token: Token to use for padding when neighborhood extends beyond sequence\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts containing neighborhood information\n",
    "    \"\"\"\n",
    "    neighborhoods = []\n",
    "    \n",
    "    for i, act_info in enumerate(tqdm(top_activations, desc=\"Extracting neighborhoods\")):\n",
    "        protein_id = act_info.protein_id\n",
    "        residue_idx = act_info.residue_idx - 1 # Use residue index, not token index\n",
    "        \n",
    "        if protein_id not in protein_sequences:\n",
    "            print(f\"Warning: Protein {protein_id} not found in sequence database\")\n",
    "            continue\n",
    "        \n",
    "        sequence = protein_sequences[protein_id]\n",
    "        \n",
    "        # Validate that residue_idx is within the sequence\n",
    "        if residue_idx >= len(sequence) or residue_idx < 0:\n",
    "            print(f\"Warning: Residue index {residue_idx} out of bounds for protein {protein_id} (length {len(sequence)})\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate desired neighborhood boundaries\n",
    "        desired_start = residue_idx - window_size\n",
    "        desired_end = residue_idx + window_size + 1\n",
    "        \n",
    "        # Calculate actual sequence boundaries\n",
    "        seq_start = max(0, desired_start)\n",
    "        seq_end = min(len(sequence), desired_end)\n",
    "        \n",
    "        # Extract the actual sequence part\n",
    "        actual_seq = sequence[seq_start:seq_end]\n",
    "        \n",
    "        # Calculate padding needed\n",
    "        left_padding = seq_start - desired_start  # How many positions we're missing on the left\n",
    "        right_padding = desired_end - seq_end     # How many positions we're missing on the right\n",
    "        \n",
    "        # Create the full neighborhood with padding\n",
    "        neighborhood_seq = (pad_token * left_padding) + actual_seq + (pad_token * right_padding)\n",
    "        \n",
    "        # The central residue is always at position window_size in the padded sequence\n",
    "        central_pos_in_neighborhood = window_size\n",
    "        \n",
    "        neighborhood_info = {\n",
    "            'rank': i + 1,\n",
    "            'activation_value': act_info.activation_value,\n",
    "            'protein_idx': act_info.protein_idx,\n",
    "            'protein_id': protein_id,\n",
    "            'token_idx': act_info.token_idx,  # Keep original token index for reference\n",
    "            'residue_idx': residue_idx,       # Actual amino acid position in sequence\n",
    "            'central_residue': sequence[residue_idx],\n",
    "            'neighborhood_seq': neighborhood_seq,\n",
    "            'neighborhood_length': len(neighborhood_seq),\n",
    "            'central_pos_in_neighborhood': central_pos_in_neighborhood,\n",
    "            'full_seq_length': len(sequence),\n",
    "            'left_padding': left_padding,\n",
    "            'right_padding': right_padding,\n",
    "            'actual_start_in_sequence': seq_start,\n",
    "            'actual_end_in_sequence': seq_end\n",
    "        }\n",
    "        \n",
    "        neighborhoods.append(neighborhood_info)\n",
    "    \n",
    "    return neighborhoods\n",
    "\n",
    "def analyze_conservation(aligned_sequences, positions_to_analyze=None):\n",
    "    \"\"\"\n",
    "    Perform basic conservation analysis on aligned sequences.\n",
    "    \n",
    "    Args:\n",
    "        aligned_sequences: List of aligned amino acid sequences\n",
    "        positions_to_analyze: List of positions to analyze (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        position_frequencies: Dict of position -> amino acid -> count\n",
    "    \"\"\"\n",
    "    if not aligned_sequences:\n",
    "        return {}\n",
    "    \n",
    "    seq_length = len(aligned_sequences[0])\n",
    "    position_frequencies = {}\n",
    "    \n",
    "    # Analyze all positions if none specified\n",
    "    if positions_to_analyze is None:\n",
    "        positions_to_analyze = list(range(seq_length))\n",
    "    \n",
    "    for pos in positions_to_analyze:\n",
    "        position_frequencies[pos] = {}\n",
    "        \n",
    "        for seq in aligned_sequences:\n",
    "            if pos < len(seq):\n",
    "                aa = seq[pos]\n",
    "                position_frequencies[pos][aa] = position_frequencies[pos].get(aa, 0) + 1\n",
    "    \n",
    "    return position_frequencies\n",
    "\n",
    "def prepare_logomaker_data(conservation_data, aligned_sequences):\n",
    "    \"\"\"\n",
    "    Convert conservation data to format needed for logomaker.\n",
    "    \n",
    "    Args:\n",
    "        conservation_data: Dict from analyze_conservation\n",
    "        aligned_sequences: List of aligned sequences\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with amino acids as rows and positions as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    if not aligned_sequences or not conservation_data:\n",
    "        return None\n",
    "    \n",
    "    seq_length = len(aligned_sequences[0])\n",
    "    \n",
    "    # All 20 standard amino acids (excluding X for now)\n",
    "    amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', \n",
    "                   'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    \n",
    "    # Use simple integer positions for logomaker (it doesn't like multi-character column names)\n",
    "    positions = list(range(seq_length))\n",
    "    \n",
    "    # Initialize count matrix with integer positions\n",
    "    count_matrix = pd.DataFrame(0, index=amino_acids, columns=positions)\n",
    "    \n",
    "    # Fill in the counts\n",
    "    for pos in range(seq_length):\n",
    "        if pos in conservation_data:\n",
    "            for aa, count in conservation_data[pos].items():\n",
    "                if aa in amino_acids:  # Skip X (padding) for cleaner logo\n",
    "                    count_matrix.loc[aa, pos] = count\n",
    "    \n",
    "    return count_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache with 10000 proteins\n",
      "{'protein_id': 'sp|Q5B136|EIF3J_EMENI', 'sequence_length': 265, 'activations': {4: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 8: tensor([[0.0000, 0.0000, 3.1250,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), 12: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7837],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1602, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), 16: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [3.0527, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), 20: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.5391, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), 24: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 28: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}}\n",
      "Loading protein sequences...\n",
      "Loaded 554961 protein sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp = load_cached_activations('/project/pi_annagreen_umass_edu/jatin/plm_circuits/acts')\n",
    "test_protein = tmp.load_protein_activations_original_format(0)\n",
    "print(test_protein)\n",
    "\n",
    "# Load protein sequences from FASTA file to get actual amino acid sequences\n",
    "FASTA_PATH = pathlib.Path(\"../../plm_interp/uniprot_sprot.fasta\")\n",
    "print(\"Loading protein sequences...\")\n",
    "protein_sequences = {}\n",
    "for rec in SeqIO.parse(FASTA_PATH, \"fasta\"):\n",
    "    if len(rec.seq) <= 1022:  # Same filter as used in the activation cache\n",
    "        protein_sequences[rec.id] = str(rec.seq)\n",
    "\n",
    "print(f\"Loaded {len(protein_sequences)} protein sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c22a8",
   "metadata": {},
   "source": [
    "# 1. getting top (protein, residue) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cff4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top 100 activations for layer 4, latent 3788...\n",
      "Using streamlined single-pass approach (one max per protein)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 10000/10000 [15:51<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "- Processed 10000 proteins\n",
      "- Found 100 top activations\n",
      "- Activation range: 5.4141 to 0.0000\n",
      "99\n",
      "ActivationInfo(activation_value=5.4140625, protein_idx=2274, token_idx=4, protein_id='sp|B1JIX0|RS17_YERPY', residue_idx=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% 1. Finding top (protein, residue) pairs\n",
    "layer_idx = 4\n",
    "latent_idx = 3788\n",
    "top_k = 100\n",
    "window_size = 10\n",
    "load_from_cache = False\n",
    "\n",
    "if load_from_cache:\n",
    "    with open(f'top_activations_layer{layer_idx}_latent{latent_idx}_top{top_k}_streamlined.pkl', 'rb') as f:\n",
    "        top_activations = pickle.load(f)\n",
    "else: \n",
    "# Find top K activations across all proteins (using streamlined approach)\n",
    "    top_activations = find_top_k_activations_streamlined(\n",
    "        tmp, layer_idx, latent_idx, k=top_k\n",
    "    )\n",
    "    with open(f'top_activations_layer{layer_idx}_latent{latent_idx}_top{top_k}_streamlined.pkl', 'wb') as f:\n",
    "        pickle.dump(top_activations, f)\n",
    "\n",
    "non_zero_activations = [act for act in top_activations if act.activation_value > 0.05]\n",
    "print(len(non_zero_activations))\n",
    "print(top_activations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78619b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting neighborhoods: 100%|██████████| 99/99 [00:00<00:00, 137268.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Residue index -1 out of bounds for protein sp|P81573|DEFD7_SPIOL (length 38)\n",
      "\n",
      "Extracted 98 neighborhoods\n",
      "\n",
      "Top 10 neighborhoods:\n",
      " 1. Act=5.4141, sp|B1JIX0|RS17_YERPY, residue_pos=2 (token=4): XXXXXXXXMT[D]QIRTLQGRVV\n",
      "    Padding: 8 left, 0 right\n",
      " 2. Act=4.3945, sp|Q1E6Q0|CMR1_COCIM, residue_pos=233 (token=235): ESDEEDEYPD[P]TITTIKPHTN\n",
      " 3. Act=4.3828, sp|Q07521|OPI6_YEAST, residue_pos=66 (token=68): SVTIKGLTGP[C]LISSSGTGSS\n",
      " 4. Act=4.3711, sp|Q92HD2|Y839_RICCN, residue_pos=46 (token=48): LPQYLANKEY[Q]KIDKQKFNSH\n",
      " 5. Act=4.3398, sp|Q6ZLB0|RSBZ1_ORYSJ, residue_pos=143 (token=145): LATVAMWRAS[G]AIHSESPLGN\n",
      " 6. Act=4.2031, sp|P54621|BUK_GEOSE, residue_pos=5 (token=7): XXXXXMQEQK[F]RILTINPGST\n",
      "    Padding: 5 left, 0 right\n",
      " 7. Act=4.1484, sp|Q3K429|MNME_PSEPF, residue_pos=451 (token=453): DDLLGRIFSS[F]CIGKXXXXXX\n",
      "    Padding: 0 left, 6 right\n",
      " 8. Act=4.1133, sp|Q87KT0|PUR9_VIBPA, residue_pos=430 (token=432): CWKVAKYVKS[N]AIVYAKGDMT\n",
      " 9. Act=4.0273, sp|Q7MN30|QUEF_VIBVY, residue_pos=146 (token=148): TQQPIVTMEG[E]CIDEQDIDIS\n",
      "10. Act=4.0117, sp|B7LFG6|TRHO_ECO55, residue_pos=5 (token=7): XXXXXMPVLH[N]RISNDALKAK\n",
      "    Padding: 5 left, 0 right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "neighborhoods = extract_neighborhoods(non_zero_activations, protein_sequences, window_size=window_size)\n",
    "\n",
    "print(f\"\\nExtracted {len(neighborhoods)} neighborhoods\")\n",
    "\n",
    "# Display first few neighborhoods\n",
    "print(\"\\nTop 10 neighborhoods:\")\n",
    "for i, neighborhood in enumerate(neighborhoods[:10]):\n",
    "    central_res = neighborhood['central_residue']\n",
    "    seq = neighborhood['neighborhood_seq']\n",
    "    central_pos = neighborhood['central_pos_in_neighborhood']\n",
    "    act_val = neighborhood['activation_value']\n",
    "    residue_pos = neighborhood['residue_idx']\n",
    "    token_pos = neighborhood['token_idx']\n",
    "    \n",
    "    # Create a visual representation with the central residue highlighted\n",
    "    seq_display = seq[:central_pos] + f\"[{central_res}]\" + seq[central_pos+1:]\n",
    "    \n",
    "    # Show both residue position (in sequence) and token position (in model)\n",
    "    print(f\"{i+1:2d}. Act={act_val:.4f}, {neighborhood['protein_id']}, residue_pos={residue_pos} (token={token_pos}): {seq_display}\")\n",
    "    \n",
    "    # Show padding info if any\n",
    "    if neighborhood['left_padding'] > 0 or neighborhood['right_padding'] > 0:\n",
    "        print(f\"    Padding: {neighborhood['left_padding']} left, {neighborhood['right_padding']} right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195c3b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 5.4141, 0.0000, 0.0000, 1.6387, 0.0000, 0.0000,\n",
      "        0.0000, 1.9424, 1.8057, 0.9424, 0.0000, 0.0000, 1.4707, 0.0000, 0.0000,\n",
      "        0.0000, 1.6035, 1.6064, 1.4385, 0.0000, 3.6445, 0.0000, 0.0000, 1.4141,\n",
      "        1.4248, 0.0000, 0.0000, 0.0000, 4.1016, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        3.5410, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1084, 0.0000, 1.3301,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.4570,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.5938, 0.0000, 0.0000, 0.0000,\n",
      "        0.8135, 0.0000, 0.0000, 0.8740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.0996, 0.0000, 0.0000, 1.0566, 0.0000, 0.0000, 0.0000,\n",
      "        4.1250, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_protein = tmp.load_protein_activations_original_format(non_zero_activations[0].protein_idx)\n",
    "print(test_protein['activations'][4][:, latent_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad8b728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(protein_sequences[non_zero_activations[0].protein_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f638c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_protein['activations'][4][:, latent_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09984c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conservation analysis for top 5 positions around center:\n",
      "Position -2 (absolute pos 8):\n",
      "  Q:  10 (10.20%)\n",
      "  V:  10 (10.20%)\n",
      "  R:   9 (9.18%)\n",
      "  A:   8 (8.16%)\n",
      "  S:   7 (7.14%)\n",
      "\n",
      "Position -1 (absolute pos 9):\n",
      "  G:  10 (10.20%)\n",
      "  S:   9 (9.18%)\n",
      "  E:   8 (8.16%)\n",
      "  T:   7 (7.14%)\n",
      "  K:   7 (7.14%)\n",
      "\n",
      "Position +0 (absolute pos 10):\n",
      "  R:  12 (12.24%)\n",
      "  A:  12 (12.24%)\n",
      "  G:   9 (9.18%)\n",
      "  E:   8 (8.16%)\n",
      "  T:   8 (8.16%)\n",
      "\n",
      "Position +1 (absolute pos 11):\n",
      "  V:  11 (11.22%)\n",
      "  A:  10 (10.20%)\n",
      "  E:   9 (9.18%)\n",
      "  L:   8 (8.16%)\n",
      "  K:   8 (8.16%)\n",
      "\n",
      "Position +2 (absolute pos 12):\n",
      "  I:  98 (100.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aligned_sequences = [n['neighborhood_seq'] for n in neighborhoods]\n",
    "# Analyze conservation patterns\n",
    "conservation_data = analyze_conservation(aligned_sequences)\n",
    "\n",
    "print(\"\\nConservation analysis for top 5 positions around center:\")\n",
    "central_pos = window_size  # The central position in our aligned sequences\n",
    "\n",
    "for offset in [-2, -1, 0, 1, 2]:\n",
    "    pos = central_pos + offset\n",
    "    if pos in conservation_data:\n",
    "        print(f\"Position {offset:+d} (absolute pos {pos}):\")\n",
    "        # Sort amino acids by frequency\n",
    "        sorted_aas = sorted(conservation_data[pos].items(), key=lambda x: x[1], reverse=True)\n",
    "        total_count = sum(conservation_data[pos].values())\n",
    "        \n",
    "        for aa, count in sorted_aas[:5]:  # Show top 5 amino acids\n",
    "            freq = count / total_count\n",
    "            print(f\"  {aa}: {count:3d} ({freq:.2%})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1505e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data for logomaker\n",
    "count_matrix = prepare_logomaker_data(conservation_data, aligned_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "433838ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  11  12  13  14  15  16  17  \\\n",
       "A   4   4   3   3   5   7   3   5   8   6  ...  10   0   7   9   9   8   4   \n",
       "C   3   0   1   1   0   1   0   0   0   4  ...   3   0   5   2   1   1   1   \n",
       "D   9   7   8   4   8   2   9   5   5   5  ...   6   0  10   7   5   5   7   \n",
       "E   6   6   6   8   7   4  10   7   6   8  ...   9   0   5  10   8   5   4   \n",
       "F   4   2   3   3   4   5   5   5   3   1  ...   3   0   2   1   5   4   5   \n",
       "G   8   3   5  11   4   8   5  11   5  10  ...   0   0   9   8   8  11  13   \n",
       "H   2   5   1   2   3   2   2   3   1   3  ...   5   0   4   0   3   2   5   \n",
       "I   5   5   7   9   5   7   6   2   2   2  ...   4  98   1   2   5   6   4   \n",
       "K   2  11   6   4   3   9   5   5   7   7  ...   8   0   4   8   5   7   5   \n",
       "L   4   7  11  12   9   4  13   7   6   4  ...   8   0   5   5   4   3   4   \n",
       "M   2   2   0   3   2   7   3   2   4   3  ...   2   0   0   3   0   0   3   \n",
       "N   2   4   4   2   6   3   5   7   5   4  ...   6   0   3   3   6   4   3   \n",
       "P   5   4   9   3   4   5   3   1   4   6  ...   0   0   3   3   2   4   6   \n",
       "Q   4   4   3   1   0   5   2   2  10   4  ...   5   0   5   3   8   4   0   \n",
       "R   9   5   4   4   7   4   4   7   9   5  ...   5   0   6   7   2  10  10   \n",
       "S   7   5   4   5   8   2   3   7   7   9  ...   3   0  13  11   6   6   4   \n",
       "T   5   4   4   2   6   2   4   6   3   7  ...   5   0   6   7   6   4   4   \n",
       "V   5   5   9   9   4  11   5   8  10   5  ...  11   0   8   5   7   6   3   \n",
       "W   0   2   0   0   2   1   4   1   1   1  ...   2   0   0   0   2   1   1   \n",
       "Y   2   3   0   4   4   4   3   3   1   4  ...   3   0   1   1   1   1   5   \n",
       "\n",
       "   18  19  20  \n",
       "A   7   8   7  \n",
       "C   0   1   1  \n",
       "D   5   2   1  \n",
       "E   5   4   3  \n",
       "F   1   3   3  \n",
       "G   6   9   9  \n",
       "H   5   1   2  \n",
       "I   5   5   8  \n",
       "K   6   8   5  \n",
       "L   7  10   6  \n",
       "M   1   1   2  \n",
       "N   5   3   6  \n",
       "P   6   5   3  \n",
       "Q   2   4   7  \n",
       "R   6   3   6  \n",
       "S   9  10   8  \n",
       "T   8   3   5  \n",
       "V   2   6   6  \n",
       "W   3   2   1  \n",
       "Y   3   4   3  \n",
       "\n",
       "[20 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# 0. sanity checks\n",
    "# -----------------------------------------------------------------------\n",
    "if count_matrix is None or count_matrix.empty:\n",
    "    raise ValueError(\"count_matrix is empty – build it first!\")\n",
    "\n",
    "# transpose so rows = positions, columns = amino acids\n",
    "logo_df = count_matrix.T            # shape: (positions, 20 AA’s)\n",
    "logo_df.index.name = 'pos'          # nice index name\n",
    "\n",
    "# make sure only standard AA columns remain & in canonical order\n",
    "aa_cols = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "logo_df = logo_df.reindex(columns=aa_cols, fill_value=0)\n",
    "\n",
    "seq_len = len(logo_df)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1. plot\n",
    "# -----------------------------------------------------------------------\n",
    "plt.rcParams['figure.dpi']  = 300\n",
    "plt.rcParams['font.size']   = 12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, seq_len * 0.5), 6))\n",
    "\n",
    "logomaker.Logo(\n",
    "    logo_df,\n",
    "    ax=ax,\n",
    "    fade_below=0.5,\n",
    "    stack_order='big_on_top',\n",
    "    color_scheme='NajafabadiEtAl2017'\n",
    ")\n",
    "\n",
    "# x‑axis labels = relative positions\n",
    "rel_labels = [f'{i - window_size:+d}' if i != window_size else '0'\n",
    "              for i in range(seq_len)]\n",
    "\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_xticklabels(rel_labels)\n",
    "ax.set_xlabel('Position relative to center')\n",
    "ax.set_ylabel('Information Content (bits)')\n",
    "ax.set_title(\n",
    "    f'Sequence Logo – Layer {layer_idx}, Latent {latent_idx}\\n'\n",
    "    f'Top {len(aligned_sequences)} Activating Neighborhoods (±{window_size} residues)',\n",
    "    pad=20, weight='bold'\n",
    ")\n",
    "\n",
    "# highlight centre residue\n",
    "ax.axvline(window_size, color='red', ls='--', lw=2, alpha=.7)\n",
    "ax.text(window_size, ax.get_ylim()[1]*0.95, 'Activating\\nResidue',\n",
    "        ha='center', va='top', color='red', weight='bold', fontsize=10)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(True, axis='y', alpha=.3)\n",
    "fig.tight_layout()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2. save & show\n",
    "# -----------------------------------------------------------------------\n",
    "png_name = f'sequence_logo_layer{layer_idx}_latent{latent_idx}_top{len(aligned_sequences)}_v2.png'\n",
    "pdf_name = f'sequence_logo_layer{layer_idx}_latent{latent_idx}_top{len(aligned_sequences)}_v2.pdf'\n",
    "fig.savefig(png_name, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "fig.savefig(pdf_name, format='pdf', bbox_inches='tight', facecolor='white')\n",
    "print(f\"Saved logo as {png_name} and {pdf_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e79bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_activations_multi_layer_vectorized(cache_system, layer_latent_dict, layer_subset=None, k=100):\n",
    "    \"\"\"\n",
    "    Vectorized approach: Process multiple layers and their specific latents simultaneously.\n",
    "    \n",
    "    Args:\n",
    "        cache_system: Your activation cache\n",
    "        layer_latent_dict: Dict mapping layer (as string) to list of latent indices\n",
    "                          e.g., {'4': [2311, 2443, 1682], '8': [1234, 5678], '12': [9876]}\n",
    "        layer_subset: Optional list of layers to process (e.g., [4, 8, 12]). \n",
    "                     If None, processes all layers in layer_latent_dict\n",
    "        k: Number of top activations to keep globally\n",
    "    \n",
    "    Returns:\n",
    "        List of ExtendedActivationInfo with layer_idx and latent_idx added\n",
    "    \"\"\"\n",
    "    # Convert string keys to integers and filter by layer_subset if provided\n",
    "    if layer_subset is not None:\n",
    "        filtered_dict = {str(layer): latents for layer, latents in layer_latent_dict.items() \n",
    "                        if int(layer) in layer_subset}\n",
    "    else:\n",
    "        filtered_dict = layer_latent_dict\n",
    "    \n",
    "    # Convert to integer keys for processing\n",
    "    layer_latent_mapping = {int(layer): latents for layer, latents in filtered_dict.items()}\n",
    "    \n",
    "    total_combinations = sum(len(latents) for latents in layer_latent_mapping.values())\n",
    "    print(f\"Finding top {k} activations across {len(layer_latent_mapping)} layers, {total_combinations} total layer-latent combinations...\")\n",
    "    \n",
    "    # Enhanced ActivationInfo to include layer and latent\n",
    "    from collections import namedtuple\n",
    "    ExtendedActivationInfo = namedtuple('ExtendedActivationInfo', \n",
    "        ['activation_value', 'protein_idx', 'token_idx', 'protein_id', 'residue_idx', 'layer_idx', 'latent_idx'])\n",
    "    \n",
    "    # Global heap for top K across all combinations\n",
    "    global_top_k_heap = []\n",
    "    \n",
    "    # Process proteins one by one to manage memory\n",
    "    for protein_idx in tqdm(range(len(cache_system.metadata['proteins'])), desc=\"Processing proteins\"):\n",
    "        try:\n",
    "            protein_data = cache_system.metadata['proteins'][protein_idx]\n",
    "            protein_id = protein_data['protein_id']\n",
    "            \n",
    "            # Load all layers for this protein at once\n",
    "            protein_layer_data = {}\n",
    "            for layer_idx, latent_indices in layer_latent_mapping.items():\n",
    "                layer_file = protein_data[f'layer_{layer_idx}_file']\n",
    "                \n",
    "                if cache_system.use_sparse:\n",
    "                    sparse_matrix = load_npz(layer_file)\n",
    "                    # Extract only the latents we care about for this layer\n",
    "                    acts_subset = sparse_matrix[:, latent_indices].toarray()  # [seq_len, num_latents_for_this_layer]\n",
    "                else:\n",
    "                    acts_np = np.load(layer_file)\n",
    "                    acts_subset = acts_np[:, latent_indices]  # [seq_len, num_latents_for_this_layer]\n",
    "                \n",
    "                protein_layer_data[layer_idx] = acts_subset\n",
    "            \n",
    "            # Vectorized processing: find max across sequence for each (layer, latent) combination\n",
    "            for layer_idx, latent_indices in layer_latent_mapping.items():\n",
    "                acts = protein_layer_data[layer_idx]  # [seq_len, num_latents_for_this_layer]\n",
    "                \n",
    "                # Find max values and indices for all latents at once\n",
    "                max_values = np.max(acts, axis=0)  # [num_latents_for_this_layer]\n",
    "                max_indices = np.argmax(acts, axis=0)  # [num_latents_for_this_layer]\n",
    "                \n",
    "                # Process each latent for this layer\n",
    "                for i, latent_idx in enumerate(latent_indices):\n",
    "                    max_activation = float(max_values[i])\n",
    "                    max_residue_idx = int(max_indices[i])\n",
    "                    \n",
    "                    activation_info = ExtendedActivationInfo(\n",
    "                        activation_value=max_activation,\n",
    "                        protein_idx=protein_idx,\n",
    "                        token_idx=max_residue_idx + 1,  # +1 for BOS token offset\n",
    "                        protein_id=protein_id,\n",
    "                        residue_idx=max_residue_idx,\n",
    "                        layer_idx=layer_idx,\n",
    "                        latent_idx=latent_idx\n",
    "                    )\n",
    "                    \n",
    "                    # Maintain global top-K\n",
    "                    if len(global_top_k_heap) < k:\n",
    "                        heapq.heappush(global_top_k_heap, (-max_activation, protein_idx, activation_info))\n",
    "                    elif max_activation > -global_top_k_heap[0][0]:\n",
    "                        heapq.heapreplace(global_top_k_heap, (-max_activation, protein_idx, activation_info))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein {protein_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Extract and sort results\n",
    "    top_k_activations = [item[2] for item in global_top_k_heap]\n",
    "    top_k_activations.sort(key=lambda x: x.activation_value, reverse=True)\n",
    "    \n",
    "    print(f\"\\nResults across {len(layer_latent_mapping)} layers, {total_combinations} layer-latent combinations:\")\n",
    "    print(f\"- Found {len(top_k_activations)} top activations\")\n",
    "    if top_k_activations:\n",
    "        print(f\"- Activation range: {top_k_activations[0].activation_value:.4f} to {top_k_activations[-1].activation_value:.4f}\")\n",
    "    \n",
    "    return top_k_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb87e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_activations_per_latent(cache_system, layer_latent_dict, layer_subset=None, k=100):\n",
    "    \"\"\"\n",
    "    Find top K activations FOR EACH latent individually.\n",
    "    Load each protein once, process all latents for that protein.\n",
    "    \n",
    "    Args:\n",
    "        cache_system: Your activation cache\n",
    "        layer_latent_dict: Dict mapping layer (as string) to list of latent indices\n",
    "        layer_subset: Optional list of layers to process\n",
    "        k: Number of top activations to keep PER latent\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping (layer_idx, latent_idx) -> list of top K activations for that latent\n",
    "    \"\"\"\n",
    "    # Convert string keys to integers and filter by layer_subset if provided\n",
    "    if layer_subset is not None:\n",
    "        filtered_dict = {str(layer): latents for layer, latents in layer_latent_dict.items() \n",
    "                        if int(layer) in layer_subset}\n",
    "    else:\n",
    "        filtered_dict = layer_latent_dict\n",
    "    \n",
    "    # Convert to integer keys for processing\n",
    "    layer_latent_mapping = {int(layer): latents for layer, latents in filtered_dict.items()}\n",
    "    \n",
    "    total_combinations = sum(len(latents) for latents in layer_latent_mapping.values())\n",
    "    print(f\"Finding top {k} activations for EACH of {total_combinations} latents across {len(layer_latent_mapping)} layers...\")\n",
    "    \n",
    "    # Enhanced ActivationInfo to include layer and latent\n",
    "    from collections import namedtuple\n",
    "    ExtendedActivationInfo = namedtuple('ExtendedActivationInfo', \n",
    "        ['activation_value', 'protein_idx', 'token_idx', 'protein_id', 'residue_idx', 'layer_idx', 'latent_idx'])\n",
    "    \n",
    "    # Create separate top-K heap for each latent\n",
    "    latent_heaps = {}\n",
    "    for layer_idx, latent_indices in layer_latent_mapping.items():\n",
    "        for latent_idx in latent_indices:\n",
    "            latent_heaps[(layer_idx, latent_idx)] = []\n",
    "    \n",
    "    # Process proteins one by one (OUTER LOOP - load each protein once)\n",
    "    for protein_idx in tqdm(range(len(cache_system.metadata['proteins'])), desc=\"Processing proteins\"):\n",
    "        try:\n",
    "            protein_data = cache_system.metadata['proteins'][protein_idx]\n",
    "            protein_id = protein_data['protein_id']\n",
    "            \n",
    "            # Load all layers for this protein at once\n",
    "            protein_layer_data = {}\n",
    "            for layer_idx, latent_indices in layer_latent_mapping.items():\n",
    "                layer_file = protein_data[f'layer_{layer_idx}_file']\n",
    "                \n",
    "                if cache_system.use_sparse:\n",
    "                    sparse_matrix = load_npz(layer_file)\n",
    "                    # Extract only the latents we care about for this layer\n",
    "                    acts_subset = sparse_matrix[:, latent_indices].toarray()  # [seq_len, num_latents_for_this_layer]\n",
    "                else:\n",
    "                    acts_np = np.load(layer_file)\n",
    "                    acts_subset = acts_np[:, latent_indices]  # [seq_len, num_latents_for_this_layer]\n",
    "                \n",
    "                protein_layer_data[layer_idx] = acts_subset\n",
    "            \n",
    "            # Process all latents for this protein (INNER LOOP - process each latent)\n",
    "            for layer_idx, latent_indices in layer_latent_mapping.items():\n",
    "                acts = protein_layer_data[layer_idx]  # [seq_len, num_latents_for_this_layer]\n",
    "                \n",
    "                # Find max values and indices for all latents at once\n",
    "                max_values = np.max(acts, axis=0)  # [num_latents_for_this_layer]\n",
    "                max_indices = np.argmax(acts, axis=0)  # [num_latents_for_this_layer]\n",
    "                \n",
    "                # Process each latent for this layer\n",
    "                for i, latent_idx in enumerate(latent_indices):\n",
    "                    max_activation = float(max_values[i])\n",
    "                    max_residue_idx = int(max_indices[i])\n",
    "                    \n",
    "                    activation_info = ExtendedActivationInfo(\n",
    "                        activation_value=max_activation,\n",
    "                        protein_idx=protein_idx,\n",
    "                        token_idx=max_residue_idx + 1,  # +1 for BOS token offset\n",
    "                        protein_id=protein_id,\n",
    "                        residue_idx=max_residue_idx,\n",
    "                        layer_idx=layer_idx,\n",
    "                        latent_idx=latent_idx\n",
    "                    )\n",
    "                    \n",
    "                    # Maintain top-K for THIS SPECIFIC latent\n",
    "                    latent_key = (layer_idx, latent_idx)\n",
    "                    heap = latent_heaps[latent_key]\n",
    "                    \n",
    "                    if len(heap) < k:\n",
    "                        heapq.heappush(heap, (-max_activation, protein_idx, activation_info))\n",
    "                    elif max_activation > -heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (-max_activation, protein_idx, activation_info))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein {protein_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Extract and sort results for each latent\n",
    "    results_per_latent = {}\n",
    "    \n",
    "    for latent_key, heap in latent_heaps.items():\n",
    "        layer_idx, latent_idx = latent_key\n",
    "        \n",
    "        # Extract results and sort by activation value (highest first)\n",
    "        latent_results = [item[2] for item in heap]\n",
    "        latent_results.sort(key=lambda x: x.activation_value, reverse=True)\n",
    "        results_per_latent[latent_key] = latent_results\n",
    "        \n",
    "        print(f\"Layer {layer_idx}, Latent {latent_idx}: {len(latent_results)} activations, \"\n",
    "              f\"range {latent_results[0].activation_value:.4f} to {latent_results[-1].activation_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCompleted! Found top {k} activations for each of {len(results_per_latent)} latents\")\n",
    "    \n",
    "    # Save results automatically\n",
    "    import pickle\n",
    "    save_path = f'../intermediate_ops/top_activations_per_latent_k{k}.pkl'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        # Convert to dictionaries to avoid pickle issues\n",
    "        save_dict = {}\n",
    "        for (layer_idx, latent_idx), activations in results_per_latent.items():\n",
    "            key_str = f\"layer_{layer_idx}_latent_{latent_idx}\"\n",
    "            save_dict[key_str] = [\n",
    "                {\n",
    "                    'activation_value': act.activation_value,\n",
    "                    'protein_idx': act.protein_idx,\n",
    "                    'token_idx': act.token_idx,\n",
    "                    'protein_id': act.protein_id,\n",
    "                    'residue_idx': act.residue_idx,\n",
    "                    'layer_idx': act.layer_idx,\n",
    "                    'latent_idx': act.latent_idx\n",
    "                }\n",
    "                for act in activations\n",
    "            ]\n",
    "        pickle.dump(save_dict, f)\n",
    "    \n",
    "    print(f\"Results automatically saved to: {save_path}\")\n",
    "    \n",
    "    return results_per_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8caf6ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4': [2311, 2443, 1682, 3351, 1690, 794, 798, 2340, 1712, 3634, 3764, 181, 443, 1474, 3651, 963, 1096, 3788, 3153, 340, 2277, 237, 3701, 3326], '8': [1575, 488, 431, 2677, 2486, 2775, 3384], '12': [899, 904, 3598, 1818, 667, 798, 803, 3751, 3239, 3626, 3498, 2477, 565, 2112, 2628, 3536, 2512, 3794, 339, 3035, 1121, 1377, 1256, 2797, 1518, 3960, 2302], '16': [1927, 391, 2446, 2450, 3092, 2836, 3994, 2588, 157, 928, 2341, 3879, 1447, 1452, 1836, 46, 1840, 1203, 1460, 3513, 2878, 1728, 3265, 3393, 708, 3141, 3665, 1236, 1880, 858, 606, 3679, 1504, 1380, 357, 2536, 3563, 2549, 1399, 3454, 2175], '20': [898, 1540, 3462, 1800, 908, 2322, 2324, 3732, 2327, 2967, 3615, 3616, 4000, 930, 2721, 1960, 3639, 441, 59, 830, 3775, 2495, 2500, 838, 2632, 1744, 3410, 3924, 4058, 1756, 3677, 3580, 4060, 2401, 1505, 1763, 3044, 2023, 2538, 878, 3567, 1395, 1276, 1533], '24': [2050, 1155, 898, 3330, 14, 2063, 2321, 1427, 1300, 2586, 3996, 1694, 1822, 3872, 2209, 929, 2848, 1828, 41, 2349, 2350, 430, 1069, 3117, 950, 3895, 3264, 1858, 2627, 2632, 1097, 328, 3016, 2253, 2254, 78, 976, 471, 88, 731, 220, 605, 2398, 1505, 3171, 4068, 1893, 3327, 231, 3946, 491, 3052, 110, 3439, 1263, 2545, 1013, 1909, 3959, 3065, 3066, 3196, 1407], '28': [896, 1095, 487, 3755, 334, 817, 116, 213, 1879]}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/layer_latent_dict_2b61.json', 'r') as f:\n",
    "    layer_latent_dict = json.load(f)\n",
    "print(layer_latent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_activations = find_top_k_activations_per_latent(\n",
    "    cache_system=tmp, \n",
    "    layer_latent_dict=layer_latent_dict,\n",
    "    layer_subset=[4, 8, 12],  # Only process these layers\n",
    "    k=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "180882d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_motif_logo_for_latent(results_per_latent, protein_sequences, layer_idx, latent_idx, window_size=10, activation_threshold=0.05, save_plots=True, show_plot=True):\n",
    "    \"\"\"\n",
    "    Generate sequence logo for a specific layer,latent using existing functions.\n",
    "    Reuses extract_neighborhoods, analyze_conservation, and logo generation code exactly as defined.\n",
    "    \n",
    "    Args:\n",
    "        results_per_latent: Dict from find_top_k_activations_per_latent() \n",
    "                           mapping (layer_idx, latent_idx) -> list of ExtendedActivationInfo\n",
    "        protein_sequences: Dict mapping protein_id to sequence string\n",
    "        layer_idx: Layer to analyze\n",
    "        latent_idx: Latent to analyze  \n",
    "        window_size: Number of residues on each side of center\n",
    "        save_plots: Whether to save PNG/PDF files\n",
    "        show_plot: Whether to display the plot\n",
    "    \n",
    "    Returns:\n",
    "        Dict with all the results\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import logomaker\n",
    "    \n",
    "    # Get activations for this specific latent\n",
    "    latent_key = (layer_idx, latent_idx)\n",
    "    if latent_key not in results_per_latent:\n",
    "        print(f\"No results found for layer {layer_idx}, latent {latent_idx}\")\n",
    "        return None\n",
    "    \n",
    "    extended_activations = results_per_latent[latent_key]\n",
    "    print(f\"Generating motif logo for layer {layer_idx}, latent {latent_idx}\")\n",
    "    print(f\"Using {len(extended_activations)} top activating positions\")\n",
    "    \n",
    "    # Convert ExtendedActivationInfo back to ActivationInfo format for existing functions\n",
    "    # AND filter by activation threshold\n",
    "    top_activations = []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    for ext_act in extended_activations:\n",
    "        if ext_act.activation_value >= activation_threshold:\n",
    "            # Create ActivationInfo with the exact same fields your existing functions expect\n",
    "            act_info = ActivationInfo(\n",
    "                activation_value=ext_act.activation_value,\n",
    "                protein_idx=ext_act.protein_idx,\n",
    "                token_idx=ext_act.token_idx,\n",
    "                protein_id=ext_act.protein_id,\n",
    "                residue_idx=ext_act.residue_idx\n",
    "            )\n",
    "            top_activations.append(act_info)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    print(f\"After filtering (threshold >= {activation_threshold}): {len(top_activations)} activations\")\n",
    "    print(f\"Filtered out {filtered_count} low-activation positions\")\n",
    "    \n",
    "    if not top_activations:\n",
    "        print(\"No activations above threshold!\")\n",
    "        return None\n",
    "    \n",
    "    if len(top_activations) < 10:\n",
    "        print(f\"Warning: Only {len(top_activations)} activations above threshold - motif may not be reliable\")\n",
    "    \n",
    "    # Now use your existing functions exactly as they are\n",
    "    print(\"Extracting neighborhoods using existing function...\")\n",
    "    neighborhoods = extract_neighborhoods(top_activations, protein_sequences, window_size=window_size)\n",
    "    \n",
    "    if not neighborhoods:\n",
    "        print(\"No valid neighborhoods found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Extracted {len(neighborhoods)} neighborhoods\")\n",
    "    \n",
    "    # Use your existing conservation analysis\n",
    "    print(\"Analyzing conservation using existing function...\")\n",
    "    aligned_sequences = [n['neighborhood_seq'] for n in neighborhoods]\n",
    "    conservation_data = analyze_conservation(aligned_sequences)\n",
    "    \n",
    "    # Use your existing logomaker data preparation\n",
    "    print(\"Preparing count matrix...\")\n",
    "    count_matrix = prepare_logomaker_data(conservation_data, aligned_sequences)\n",
    "    \n",
    "    if count_matrix is None or count_matrix.empty:\n",
    "        print(\"No count matrix generated!\")\n",
    "        return None\n",
    "    \n",
    "    # Use your existing logo generation code exactly\n",
    "    print(\"Generating sequence logo using existing code...\")\n",
    "    \n",
    "    # -----------------------------------------------------------------------\n",
    "    # Your exact logo generation code (unchanged)\n",
    "    # -----------------------------------------------------------------------\n",
    "    \n",
    "    # transpose so rows = positions, columns = amino acids\n",
    "    logo_df = count_matrix.T            # shape: (positions, 20 AA's)\n",
    "    logo_df.index.name = 'pos'          # nice index name\n",
    "\n",
    "    # make sure only standard AA columns remain & in canonical order\n",
    "    aa_cols = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "    logo_df = logo_df.reindex(columns=aa_cols, fill_value=0)\n",
    "\n",
    "    seq_len = len(logo_df)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # 1. plot\n",
    "    # -----------------------------------------------------------------------\n",
    "    plt.rcParams['figure.dpi']  = 300\n",
    "    plt.rcParams['font.size']   = 12\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(8, seq_len * 0.5), 6))\n",
    "\n",
    "    logomaker.Logo(\n",
    "        logo_df,\n",
    "        ax=ax,\n",
    "        fade_below=0.5,\n",
    "        stack_order='big_on_top',\n",
    "        color_scheme='NajafabadiEtAl2017'\n",
    "    )\n",
    "\n",
    "    # x‑axis labels = relative positions\n",
    "    rel_labels = [f'{i - window_size:+d}' if i != window_size else '0'\n",
    "                  for i in range(seq_len)]\n",
    "\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(rel_labels)\n",
    "    ax.set_xlabel('Position relative to center')\n",
    "    ax.set_ylabel('Information Content (bits)')\n",
    "    ax.set_title(\n",
    "        f'Sequence Logo – Layer {layer_idx}, Latent {latent_idx}\\n'\n",
    "        f'Top {len(aligned_sequences)} Activating Neighborhoods (±{window_size} residues)',\n",
    "        pad=20, weight='bold'\n",
    "    )\n",
    "\n",
    "    # highlight centre residue\n",
    "    ax.axvline(window_size, color='red', ls='--', lw=2, alpha=.7)\n",
    "    ax.text(window_size, ax.get_ylim()[1]*0.95, 'Activating\\nResidue',\n",
    "            ha='center', va='top', color='red', weight='bold', fontsize=10)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(True, axis='y', alpha=.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # 2. save & show (using your naming convention)\n",
    "    # -----------------------------------------------------------------------\n",
    "    if save_plots:\n",
    "        png_name = f'../results/sequence_logos/sequence_logo_layer{layer_idx}_latent{latent_idx}_top{len(aligned_sequences)}.png'\n",
    "        # pdf_name = f'../results/sequence_logos/sequence_logo_layer{layer_idx}_latent{latent_idx}_top{len(aligned_sequences)}.pdf'\n",
    "        fig.savefig(png_name, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        # fig.savefig(pdf_name, format='pdf', bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Saved logo as {png_name} and {pdf_name}\")\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    # Return all results for further analysis\n",
    "    results = {\n",
    "        'layer_idx': layer_idx,\n",
    "        'latent_idx': latent_idx,\n",
    "        'top_activations': top_activations,\n",
    "        'neighborhoods': neighborhoods,\n",
    "        'aligned_sequences': aligned_sequences,\n",
    "        'conservation_data': conservation_data,\n",
    "        'count_matrix': count_matrix,\n",
    "        'logo_df': logo_df,\n",
    "        'figure': fig\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Complete! Generated logo for layer {layer_idx}, latent {latent_idx}\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layer_latent_dict:\n",
    "    for latent in layer_latent_dict[layer]:\n",
    "        print(layer, latent)\n",
    "        print(type(layer), type(latent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cded204",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layer_latent_dict:\n",
    "    for latent in layer_latent_dict[layer]:\n",
    "        results = generate_motif_logo_for_latent(\n",
    "            results_per_latent=top_activations, \n",
    "            protein_sequences=protein_sequences,\n",
    "            layer_idx=int(layer), \n",
    "            latent_idx=int(latent),\n",
    "            window_size=10, \n",
    "            show_plot=False,\n",
    "        )\n",
    "# results = generate_motif_logo_for_latent(\n",
    "#     results_per_latent=top_activations, \n",
    "#     protein_sequences=protein_sequences,\n",
    "#     layer_idx=4, \n",
    "#     latent_idx=3788,\n",
    "#     window_size=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f7768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ffc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert namedtuples to dictionaries before saving\n",
    "top_activations_dict = []\n",
    "for act in top_activations:\n",
    "    act_dict = {\n",
    "        'activation_value': act.activation_value,\n",
    "        'protein_idx': act.protein_idx,\n",
    "        'token_idx': act.token_idx,\n",
    "        'protein_id': act.protein_id,\n",
    "        'residue_idx': act.residue_idx,\n",
    "        'layer_idx': act.layer_idx,\n",
    "        'latent_idx': act.latent_idx\n",
    "    }\n",
    "    top_activations_dict.append(act_dict)\n",
    "\n",
    "# Now save the dictionaries\n",
    "with open(f'../intermediate_ops/top_activations_layers_4_8_12_latentdict_top100.pkl', 'wb') as f:\n",
    "    pickle.dump(top_activations_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8339786b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "674804dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.ExtendedActivationInfo'>: it's not the same object as __main__.ExtendedActivationInfo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m ExtendedActivationInfo \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtendedActivationInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m         [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidue_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../intermediate_ops/top_activations_layers_4_8_12_latentdict_top100.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.ExtendedActivationInfo'>: it's not the same object as __main__.ExtendedActivationInfo"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "ExtendedActivationInfo = namedtuple('ExtendedActivationInfo', \n",
    "        ['activation_value', 'protein_idx', 'token_idx', 'protein_id', 'residue_idx', 'layer_idx', 'latent_idx'])\n",
    "with open(f'../intermediate_ops/top_activations_layers_4_8_12_latentdict_top100.pkl', 'wb') as f:\n",
    "    pickle.dump(top_activations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb4fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
