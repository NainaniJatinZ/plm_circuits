{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6dc794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython\n",
      "Set autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reconcile AUROC and MWU on a common dataset with length-matched negatives.\n",
    "\n",
    "You provide a callback that maps (sequence_string, layer, unit) -> 1D array\n",
    "of per-token activations. The script will aggregate to a per-protein score\n",
    "(max / top-q% mean / log-sum-exp / mean), then compute:\n",
    "- AUROC + stratified bootstrap 95% CI\n",
    "- Mann–Whitney U (one-sided, greater) p-value\n",
    "- Verify AUC == U / (n_pos * n_neg)\n",
    "- BH–FDR q-values across the tested (latent,domain,aggregator) rows\n",
    "- Length correlation diagnostics\n",
    "- Plots per-aggregator\n",
    "\n",
    "Outputs saved into ./reconcile_out/\n",
    "\"\"\"\n",
    "\n",
    "import os, json, math, pathlib, sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Callable, Dict, List, Tuple, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import mannwhitneyu, spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# -------------------------- User configuration --------------------------\n",
    "\n",
    "import sys\n",
    "sys.path.append('../plm_circuits')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from Bio import SeqIO\n",
    "import pathlib\n",
    "import heapq\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import logomaker\n",
    "import matplotlib.pyplot as plt\n",
    "# Import utility functions\n",
    "from helpers.utils import load_esm, load_sae_prot, cleanup_cuda\n",
    "from hook_manager import SAEHookProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542eaec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device and load models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load ESM-2 model\n",
    "esm_transformer, batch_converter, esm2_alphabet = load_esm(33, device=device)\n",
    "\n",
    "# Load SAEs for multiple layers\n",
    "main_layers = [4, 8, 12, 16, 20, 24, 28]\n",
    "saes = []\n",
    "for layer in main_layers:\n",
    "    sae_model = load_sae_prot(ESM_DIM=1280, SAE_DIM=4096, LAYER=layer, device=device)\n",
    "    saes.append(sae_model)\n",
    "\n",
    "layer_2_saelayer = {layer: layer_idx for layer_idx, layer in enumerate(main_layers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95e429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Swiss-Prot/Reviewed FASTA (negatives sampling pool)\n",
    "FASTA_SPROT = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/uniprot_sprot.fasta\"       # <-- set this\n",
    "\n",
    "# Targets: (layer, unit, domain_id, pos_fasta_path) for each candidate\n",
    "TARGETS = [\n",
    "    (8, 488,  \"IPR029058\", \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029058.fasta\"),\n",
    "    (16, 1166, \"PF13589\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF13589.fasta\"),\n",
    "    (20, 2311, \"PF13589\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF13589.fasta\"),\n",
    "    (12, 1082, \"PF00867\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF00867.fasta\"),\n",
    "]\n",
    "\n",
    "# Aggregators to evaluate\n",
    "AGGREGATORS = [\"max\", \"topq\", \"mean\", \"topk\"]\n",
    "\n",
    "# Aggregator hyperparams\n",
    "TOPQ = 0.01     # top 1%\n",
    "LSE_TAU = 2.0   # temperature for log-sum-exp\n",
    "\n",
    "# Sampling & bootstrap\n",
    "MAX_SEQ_LEN     = 1022          # amino acids\n",
    "MAX_POS_SAMPLES = None          # cap positives; None => use all positives available\n",
    "NEG_MATCH_STRAT = \"length\"      # \"length\" or \"random\"\n",
    "N_LENGTH_BINS   = 12            # bins for length-matched sampling\n",
    "SEED            = 42\n",
    "BOOT_N          = 3000          # bootstrap iterations for CI\n",
    "ALTERNATIVE     = \"greater\"     # MWU one-sided; higher score => positive\n",
    "\n",
    "# Multiple testing\n",
    "MT_METHOD       = \"bh\"          # \"bh\" (FDR) or \"bonferroni\"\n",
    "ALPHA           = 0.05\n",
    "\n",
    "# Optional: demo mode to test the pipeline without a model (synthetic activations)\n",
    "DEMO_MODE       = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9fe4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define targets after confirming files exist\n",
    "TARGETS = [\n",
    "    # metx \n",
    "    (8, 488,  \"IPR029058\", \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029058.fasta\"),\n",
    "    (8, 2677, \"IPR036188\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_circuits/data/protein-matching-IPR036188.fasta\"),\n",
    "    (8, 2775, \"IPR009014\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR009014.fasta\"),\n",
    "    (8, 2166, \"IPR024072\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR024072.fasta\"),\n",
    "    (12, 2112, \"IPR029058\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029058.fasta\"),\n",
    "    (12, 3536, \"IPR029063\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029063.fasta\"),\n",
    "    (12, 1256, \"IPR016181\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR016181.fasta\"),\n",
    "    (12, 2797, \"IPR013785\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR013785.fasta\"),\n",
    "    (12, 3794, \"IPR029063\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029063.fasta\"),\n",
    "    (12, 3035, \"IPR036322\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036322.fasta\"),\n",
    "\n",
    "\n",
    "    # top2\n",
    "    (12, 1082, \"PF00867\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF00867.fasta\"),\n",
    "    (12, 2472, \"IPR036961\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036961.fasta\"),\n",
    "    (12, 3943, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (12, 1796, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (12, 1204, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (12, 1145, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (16, 1166, \"PF13589\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF13589.fasta\"),\n",
    "    (16, 3077, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (16, 1353, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (16, 1597, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (16, 1814, \"IPR036890\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR036890.fasta\"),\n",
    "    (16, 3994, \"PF13589\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF13589.fasta\"),\n",
    "    (20, 2311, \"PF13589\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF13589.fasta\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd4a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All FASTA files exist!\n"
     ]
    }
   ],
   "source": [
    "# Check that all FASTA files exist\n",
    "import os\n",
    "\n",
    "# First check Swiss-Prot FASTA exists\n",
    "if not os.path.exists(FASTA_SPROT):\n",
    "    raise FileNotFoundError(f\"Swiss-Prot FASTA file not found at: {FASTA_SPROT}\")\n",
    "\n",
    "# Check each target's FASTA file exists\n",
    "for layer, unit, domain_id, fasta_path in TARGETS:\n",
    "    if not os.path.exists(fasta_path):\n",
    "        raise FileNotFoundError(f\"FASTA file not found for {domain_id} at: {fasta_path}\")\n",
    "\n",
    "print(\"All FASTA files exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dedce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_activations(seq: str, layer_idx: int, unit_idx: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a 1D numpy array of per-token activations for 'unit' at 'layer' on 'seq'.\n",
    "    You should plug in your ESM/SAE hook here and return activations on non-padding tokens.\n",
    "\n",
    "    Example signature in your codebase:\n",
    "        acts = get_sae_token_acts(seq, layer, unit)  # shape [L]\n",
    "        return acts.astype(np.float32)\n",
    "\n",
    "    DEMO_MODE returns synthetic motif-like activations.\n",
    "    \"\"\"\n",
    "    \"\"\"Return max activation of *one neuron* over the sequence.\"\"\"\n",
    "    sae_layer = saes[layer_2_saelayer[layer_idx]]\n",
    "    _, _, toks = batch_converter([(1, seq)])\n",
    "    toks, mask = toks.to(device), (toks != esm2_alphabet.padding_idx).to(device)\n",
    "\n",
    "    hook = SAEHookProt(\n",
    "        sae          = sae_layer,\n",
    "        mask_BL         = mask,\n",
    "        cache_latents=True,\n",
    "        layer_is_lm     = False,\n",
    "        calc_error   = True,\n",
    "        use_error    = True,\n",
    "    )\n",
    "    h = esm_transformer.esm.encoder.layer[layer_idx].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        _ = esm_transformer.predict_contacts(toks, mask)[0]\n",
    "    h.remove(); torch.cuda.empty_cache()\n",
    "\n",
    "    return sae_layer.feature_acts[:, unit_idx].detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ccbf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- You must implement this callback -------------------\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# ------------------------------ FASTA IO ---------------------------------\n",
    "\n",
    "def parse_fasta(path: str, max_len: int = MAX_SEQ_LEN) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns list of (id, seq) with len(seq) <= max_len.\n",
    "    Requires Biopython if available; else, uses a simple fallback parser.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    try:\n",
    "        from Bio import SeqIO  # type: ignore\n",
    "        for rec in SeqIO.parse(path, \"fasta\"):\n",
    "            s = str(rec.seq)\n",
    "            if len(s) <= max_len:\n",
    "                entries.append((rec.id, s))\n",
    "    except Exception:\n",
    "        # Minimal FASTA reader\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ident, buf = None, []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.startswith(\">\"):\n",
    "                    if ident is not None:\n",
    "                        seq = \"\".join(buf)\n",
    "                        if len(seq) <= max_len:\n",
    "                            entries.append((ident, seq))\n",
    "                    ident = line[1:].split()[0]\n",
    "                    buf = []\n",
    "                else:\n",
    "                    buf.append(line)\n",
    "            if ident is not None:\n",
    "                seq = \"\".join(buf)\n",
    "                if len(seq) <= max_len:\n",
    "                    entries.append((ident, seq))\n",
    "    return entries\n",
    "\n",
    "# --------------------------- Aggregators ---------------------------------\n",
    "\n",
    "def agg_max(tok_acts: np.ndarray) -> float:\n",
    "    return float(np.max(tok_acts)) if tok_acts.size else float(\"nan\")\n",
    "\n",
    "def agg_topq(tok_acts: np.ndarray, q: float = TOPQ) -> float:\n",
    "    if tok_acts.size == 0: return float(\"nan\")\n",
    "    k = max(1, int(math.ceil(q * tok_acts.size)))\n",
    "    # top-k mean via partial sort\n",
    "    part = np.partition(tok_acts, -k)[-k:]\n",
    "    return float(np.mean(part))\n",
    "\n",
    "def agg_lse(tok_acts: np.ndarray, tau: float = LSE_TAU) -> float:\n",
    "    if tok_acts.size == 0: return float(\"nan\")\n",
    "    a = tok_acts / max(1e-8, tau)\n",
    "    m = float(np.max(a))\n",
    "    return float(m + np.log(np.exp(a - m).sum()))\n",
    "\n",
    "def agg_mean(tok_acts: np.ndarray) -> float:\n",
    "    return float(np.mean(tok_acts)) if tok_acts.size else float(\"nan\")\n",
    "\n",
    "\n",
    "def agg_lme(tok_acts, tau=2.0):  # log-mean-exp (length neutral)\n",
    "    if tok_acts.size == 0: return float(\"nan\")\n",
    "    a = tok_acts / max(1e-8, tau)\n",
    "    m = float(np.max(a))\n",
    "    return float(m + np.log(np.exp(a - m).sum()) - np.log(tok_acts.size))\n",
    "\n",
    "def agg_topk(tok_acts, K=64):     # fixed top-K mean (length neutral)\n",
    "    if tok_acts.size == 0: return float(\"nan\")\n",
    "    k = min(K, tok_acts.size)\n",
    "    return float(np.partition(tok_acts, -k)[-k:].mean())\n",
    "\n",
    "AGG_FUNCS = {\n",
    "    \"max\": agg_max,\n",
    "    \"topq\": lambda x: agg_topq(x, TOPQ),\n",
    "    \"lse\": lambda x: agg_lse(x, LSE_TAU),\n",
    "    \"mean\": agg_mean,\n",
    "    \"lme\": lambda x: agg_lme(x),\n",
    "    \"topk\": lambda x: agg_topk(x),\n",
    "}\n",
    "\n",
    "# ------------------------- Metrics & bootstrap ---------------------------\n",
    "\n",
    "def stratified_boot_auc(y: np.ndarray, s: np.ndarray, B: int = BOOT_N, seed: int = SEED,\n",
    "                        show_progress: bool = False, desc: str = \"bootstrap\") -> Tuple[float, Tuple[float, float]]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y); s = np.asarray(s)\n",
    "    pos = s[y==1]; neg = s[y==0]\n",
    "    n1, n0 = pos.size, neg.size\n",
    "    aucs = np.empty(B, dtype=np.float64)\n",
    "    iterator = range(B)\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, leave=False, dynamic_ncols=True, desc=desc, unit=\"boot\")\n",
    "    for b in iterator:\n",
    "        pos_b = pos[rng.integers(0, n1, n1)]\n",
    "        neg_b = neg[rng.integers(0, n0, n0)]\n",
    "        y_b = np.r_[np.ones(n1, int), np.zeros(n0, int)]\n",
    "        s_b = np.r_[pos_b, neg_b]\n",
    "        aucs[b] = roc_auc_score(y_b, s_b)\n",
    "    lo, hi = np.percentile(aucs, [2.5, 97.5])\n",
    "    auc_hat = roc_auc_score(y, s)\n",
    "    return auc_hat, (float(lo), float(hi))\n",
    "\n",
    "\n",
    "def mwu_test(pos_scores: np.ndarray, neg_scores: np.ndarray, alternative: str = \"greater\"):\n",
    "    res = mannwhitneyu(pos_scores, neg_scores, alternative=alternative, method=\"auto\")\n",
    "    U = float(res.statistic)\n",
    "    p = float(res.pvalue)\n",
    "    auc_from_U = U / (pos_scores.size * neg_scores.size)\n",
    "    return U, p, auc_from_U\n",
    "\n",
    "def benjamini_hochberg(pvals: np.ndarray) -> np.ndarray:\n",
    "    p = np.asarray(pvals, dtype=float)\n",
    "    m = p.size\n",
    "    order = np.argsort(p)                 # ascending p\n",
    "    q = np.empty_like(p)\n",
    "    cummin = 1.0\n",
    "    # Traverse from largest p to smallest (step-down)\n",
    "    for i in range(m-1, -1, -1):\n",
    "        idx = order[i]\n",
    "        rank = i + 1\n",
    "        val = p[idx] * m / rank\n",
    "        cummin = min(cummin, val)\n",
    "        q[idx] = cummin\n",
    "    return np.clip(q, 0.0, 1.0)\n",
    "\n",
    "# ------------------------- Length matching utils -------------------------\n",
    "\n",
    "def quantile_bins(lengths: np.ndarray, n_bins: int) -> np.ndarray:\n",
    "    edges = np.quantile(lengths, np.linspace(0, 1, n_bins+1))\n",
    "    # ensure strictly increasing\n",
    "    for i in range(1, len(edges)):\n",
    "        if edges[i] <= edges[i-1]:\n",
    "            edges[i] = edges[i-1] + 1e-6\n",
    "    return edges\n",
    "\n",
    "def sample_length_matched_negatives(pos_lengths: np.ndarray, neg_pool_lengths: np.ndarray, rng, n_bins: int = 12) -> np.ndarray:\n",
    "    edges = quantile_bins(np.r_[pos_lengths, neg_pool_lengths], n_bins)\n",
    "    idx_pool = np.arange(neg_pool_lengths.size)\n",
    "    chosen = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = edges[i], edges[i+1]\n",
    "        pos_in_bin = ((pos_lengths >= lo) & (pos_lengths <= hi)).sum()\n",
    "        if pos_in_bin == 0: \n",
    "            continue\n",
    "        cand = idx_pool[(neg_pool_lengths >= lo) & (neg_pool_lengths <= hi)]\n",
    "        if cand.size == 0:\n",
    "            continue\n",
    "        take = min(pos_in_bin, cand.size)\n",
    "        chosen.append(rng.choice(cand, size=take, replace=False))\n",
    "    if not chosen:\n",
    "        return np.array([], dtype=int)\n",
    "    return np.concatenate(chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_BOOT_PROGRESS = True  # set False if too chatty\n",
    "\n",
    "def run_experiment(\n",
    "    targets: List[Tuple[int,int,str,str]],\n",
    "    fasta_sprot: str,\n",
    "    aggregators: List[str] = AGGREGATORS,\n",
    "    neg_match: str = NEG_MATCH_STRAT,\n",
    "    n_bins: int = N_LENGTH_BINS,\n",
    "    max_seq_len: int = MAX_SEQ_LEN,\n",
    "    max_pos_samples: int = MAX_POS_SAMPLES,\n",
    "    seed: int = SEED,\n",
    "    boot_n: int = BOOT_N,\n",
    "    mt_method: str = MT_METHOD,\n",
    "    alpha: float = ALPHA,\n",
    "    alternative: str = ALTERNATIVE,\n",
    "    outdir: str = \"./reconcile_out\"\n",
    "):\n",
    "    out = pathlib.Path(outdir); out.mkdir(parents=True, exist_ok=True)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Load negative pool (Swiss-Prot / reviewed)\n",
    "    neg_pool = parse_fasta(fasta_sprot, max_len=max_seq_len)\n",
    "    if len(neg_pool) == 0:\n",
    "        raise RuntimeError(f\"No sequences loaded from {fasta_sprot}\")\n",
    "    neg_pool_ids = np.array([pid for pid,_ in neg_pool], dtype=object)\n",
    "    neg_pool_seqs = np.array([seq for _,seq in neg_pool], dtype=object)\n",
    "    neg_pool_lens = np.array([len(s) for s in neg_pool_seqs], dtype=int)\n",
    "\n",
    "    rows = []\n",
    "    target_bar = tqdm(targets, desc=\"targets\", unit=\"latent\", dynamic_ncols=True)\n",
    "\n",
    "    for (layer, unit, domain, pos_fasta) in target_bar:\n",
    "        target_bar.set_postfix_str(f\"L{layer}.{unit} {domain}\")\n",
    "\n",
    "        # Positives\n",
    "        pos_entries = parse_fasta(pos_fasta, max_len=max_seq_len)\n",
    "        if len(pos_entries) == 0:\n",
    "            print(f\"[WARN] No positives loaded from {pos_fasta}; skipping {domain}.\")\n",
    "            continue\n",
    "        if max_pos_samples is not None and len(pos_entries) > max_pos_samples:\n",
    "            pos_entries = list(rng.choice(pos_entries, size=max_pos_samples, replace=False))\n",
    "        pos_ids  = np.array([pid for pid,_ in pos_entries], dtype=object)\n",
    "        pos_seqs = np.array([seq for _,seq in pos_entries], dtype=object)\n",
    "        pos_lens = np.array([len(s) for s in pos_seqs], dtype=int)\n",
    "\n",
    "        # Exclude positives from negative pool\n",
    "        mask_not_pos = ~np.isin(neg_pool_ids, pos_ids)\n",
    "        neg_ids_pool  = neg_pool_ids[mask_not_pos]\n",
    "        neg_seqs_pool = neg_pool_seqs[mask_not_pos]\n",
    "        neg_lens_pool = neg_pool_lens[mask_not_pos]\n",
    "\n",
    "        # Length-matched negatives\n",
    "        if neg_match == \"length\":\n",
    "            idx = sample_length_matched_negatives(pos_lens, neg_lens_pool, rng, n_bins=n_bins)\n",
    "            if idx.size < len(pos_seqs):\n",
    "                # if under-filled, top up by random (without overlap)\n",
    "                extra = rng.choice(np.setdiff1d(np.arange(len(neg_seqs_pool)), idx), \n",
    "                                   size=len(pos_seqs)-idx.size, replace=False)\n",
    "                idx = np.concatenate([idx, extra])\n",
    "        else:\n",
    "            idx = rng.choice(np.arange(len(neg_seqs_pool)), size=len(pos_seqs), replace=False)\n",
    "\n",
    "        neg_ids  = neg_ids_pool[idx]\n",
    "        neg_seqs = neg_seqs_pool[idx]\n",
    "        neg_lens = neg_lens_pool[idx]\n",
    "\n",
    "        # -------- compute token activations ONCE per sequence (with progress) --------\n",
    "        pos_tok_acts = []\n",
    "        for s in tqdm(pos_seqs, desc=f\"L{layer}.{unit} {domain}  pos seqs\", unit=\"seq\", leave=False, dynamic_ncols=True):\n",
    "            pos_tok_acts.append(compute_token_activations(s, layer, unit))\n",
    "        neg_tok_acts = []\n",
    "        for s in tqdm(neg_seqs, desc=f\"L{layer}.{unit} {domain}  neg seqs\", unit=\"seq\", leave=False, dynamic_ncols=True):\n",
    "            neg_tok_acts.append(compute_token_activations(s, layer, unit))\n",
    "\n",
    "        # Precompute aggregations for each aggregator from the cached token acts\n",
    "        for agg in aggregators:\n",
    "            func = AGG_FUNCS[agg]\n",
    "            pos_scores = np.array([func(x) for x in pos_tok_acts], dtype=np.float64)\n",
    "            neg_scores = np.array([func(x) for x in neg_tok_acts], dtype=np.float64)\n",
    "\n",
    "            y = np.r_[np.ones(pos_scores.size, dtype=int), np.zeros(neg_scores.size, dtype=int)]\n",
    "            s = np.r_[pos_scores, neg_scores]\n",
    "\n",
    "            # Metrics (with optional bootstrap bar)\n",
    "            auc, (lo, hi) = stratified_boot_auc(y, s, B=boot_n, seed=seed,\n",
    "                                                show_progress=SHOW_BOOT_PROGRESS,\n",
    "                                                desc=f\"boot L{layer}.{unit} {domain} [{agg}]\")\n",
    "            U, p, aucU = mwu_test(pos_scores, neg_scores, alternative=alternative)\n",
    "            agree = abs(auc - aucU) < 1e-6\n",
    "\n",
    "            rho_pos, _ = spearmanr(pos_scores, pos_lens)\n",
    "            rho_neg, _ = spearmanr(neg_scores, neg_lens)\n",
    "\n",
    "            rows.append({\n",
    "                \"layer\": layer, \"unit\": unit, \"domain\": domain, \"aggregator\": agg,\n",
    "                \"n_pos\": int(pos_scores.size), \"n_neg\": int(neg_scores.size),\n",
    "                \"auc\": float(auc), \"ci_lo\": float(lo), \"ci_hi\": float(hi),\n",
    "                \"U\": float(U), \"p_mwu\": float(p), \"auc_from_U\": float(aucU), \"auc_matches_U\": bool(agree),\n",
    "                \"rho_len_pos\": float(rho_pos), \"rho_len_neg\": float(rho_neg),\n",
    "            })\n",
    "\n",
    "    # ---- assemble & save (unchanged) ----\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No results produced; check your inputs.\")\n",
    "        return\n",
    "\n",
    "    if MT_METHOD.lower() == \"bh\":\n",
    "        df[\"qval\"] = benjamini_hochberg(df[\"p_mwu\"].values)\n",
    "        mt_label = \"BH-FDR qval\"\n",
    "    elif MT_METHOD.lower() == \"bonferroni\":\n",
    "        m = df.shape[0]\n",
    "        df[\"qval\"] = np.minimum(1.0, df[\"p_mwu\"].values * m)\n",
    "        mt_label = \"Bonferroni-adjusted p\"\n",
    "    else:\n",
    "        df[\"qval\"] = df[\"p_mwu\"]; mt_label = \"p (uncorrected)\"\n",
    "\n",
    "    out_csv = pathlib.Path(outdir) / \"results_reconcile.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(\"\\n=== Summary (sorted by AUC) ===\")\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(df.sort_values([\"aggregator\", \"auc\"], ascending=[True, False]).to_string(index=False,\n",
    "              formatters={\"auc\":\"{:.3f}\".format, \"ci_lo\":\"{:.3f}\".format, \"ci_hi\":\"{:.3f}\".format,\n",
    "                          \"p_mwu\":\"{:.2e}\".format, \"qval\":\"{:.2e}\".format}))\n",
    "\n",
    "    # plots (unchanged)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        for agg in sorted(df[\"aggregator\"].unique()):\n",
    "            sub = df[df[\"aggregator\"] == agg].sort_values(\"auc\", ascending=True)\n",
    "            yv = np.arange(sub.shape[0])\n",
    "            plt.figure(figsize=(6.0, max(2.5, 0.35*len(yv))))\n",
    "            plt.barh(yv, sub[\"auc\"].values, color=\"tab:blue\", edgecolor=\"black\", height=0.6)\n",
    "            xerr = np.vstack([sub[\"auc\"].values - sub[\"ci_lo\"].values,\n",
    "                              sub[\"ci_hi\"].values - sub[\"auc\"].values])\n",
    "            plt.errorbar(sub[\"auc\"].values, yv, xerr=xerr, fmt=\"none\", ecolor=\"black\", capsize=3, lw=1)\n",
    "            plt.axvline(0.5, color=\"grey\", ls=\"--\", lw=1)\n",
    "            labels = [f\"L{L}.{U}\\n{D}\" for L,U,D in zip(sub[\"layer\"], sub[\"unit\"], sub[\"domain\"])]\n",
    "            plt.yticks(yv, labels, fontsize=9)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.xlabel(\"AUROC (higher = more selective)\")\n",
    "            plt.title(f\"Selectivity by {agg} (CI = 95% stratified bootstrap)\\n{mt_label}: see CSV\", fontsize=11)\n",
    "            plt.tight_layout()\n",
    "            out_png = pathlib.Path(outdir) / f\"plot_{agg}.png\"\n",
    "            plt.savefig(out_png, dpi=200)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "    meta = {\n",
    "        \"fasta_sprot\": fasta_sprot,\n",
    "        \"targets\": TARGETS,\n",
    "        \"aggregators\": AGGREGATORS,\n",
    "        \"params\": {\n",
    "            \"TOPQ\": TOPQ, \"LSE_TAU\": LSE_TAU, \"MAX_SEQ_LEN\": MAX_SEQ_LEN,\n",
    "            \"NEG_MATCH_STRAT\": NEG_MATCH_STRAT, \"N_LENGTH_BINS\": N_LENGTH_BINS,\n",
    "            \"SEED\": SEED, \"BOOT_N\": BOOT_N, \"ALTERNATIVE\": ALTERNATIVE,\n",
    "            \"MT_METHOD\": MT_METHOD, \"ALPHA\": ALPHA,\n",
    "        },\n",
    "    }\n",
    "    with open(pathlib.Path(outdir) / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved: {out_csv}\")\n",
    "    print(f\"Plots saved under: {outdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c3628db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3f30a39fd4ccd9c94697e2b46a033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "targets:   0%|          | 0/1 [00:00<?, ?latent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5dac1988f84d4abeb41c28e4a30af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L12.1082 PF00867  pos seqs:   0%|          | 0/220 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4c4257f4034abd93238df713b253b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L12.1082 PF00867  neg seqs:   0%|          | 0/223 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039339c8938e4ab0afb309a7f002f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [max]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c7877246a8449896311129b58db91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [topq]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53983b22334a4ce3a3724687820414aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [mean]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0a0734faf0481f80c0873f8fd7b65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [topk]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary (sorted by AUC) ===\n",
      " layer  unit  domain aggregator  n_pos  n_neg   auc ci_lo ci_hi       U    p_mwu  auc_from_U  auc_matches_U  rho_len_pos  rho_len_neg     qval\n",
      "    12  1082 PF00867        max    220    223 0.998 0.995 1.000 48984.5 6.38e-74    0.998461           True     0.688171     0.063035 2.55e-73\n",
      "    12  1082 PF00867       mean    220    223 0.585 0.527 0.643 28702.5 9.79e-04    0.585049           True     0.579286     0.040839 9.79e-04\n",
      "    12  1082 PF00867       topk    220    223 0.723 0.671 0.771 35447.5 2.68e-16    0.722534           True     0.736398     0.153643 3.58e-16\n",
      "    12  1082 PF00867       topq    220    223 0.996 0.988 1.000 48848.5 3.99e-73    0.995689           True    -0.061831     0.025616 7.97e-73\n",
      "\n",
      "Saved: reconcile_out/results_reconcile.csv\n",
      "Plots saved under: ./reconcile_out\n"
     ]
    }
   ],
   "source": [
    "TARGETS = [(12, 1082, \"PF00867\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF00867.fasta\")]\n",
    "\n",
    "run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=MAX_POS_SAMPLES,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method=MT_METHOD,\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a056c11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39513bb26031494dbbab76b8c388783a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "targets:   0%|          | 0/1 [00:00<?, ?latent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2e37954cef4a98aa1ccff074de2dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L8.2677 IPR036188  pos seqs:   0%|          | 0/2000 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ffc3989abb47fea725b1fa14e74c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L8.2677 IPR036188  neg seqs:   0%|          | 0/2051 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7962a03a4746bf961fc44c9db185e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.2677 IPR036188 [max]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5870ef03e94564984e574ebac55331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.2677 IPR036188 [topq]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95097ecb568b4b30a45971f839172b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.2677 IPR036188 [mean]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcddcddebaf4a8d8b7b0a22e685a105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.2677 IPR036188 [topk]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary (sorted by AUC) ===\n",
      " layer  unit    domain aggregator  n_pos  n_neg   auc ci_lo ci_hi         U    p_mwu  auc_from_U  auc_matches_U  rho_len_pos  rho_len_neg     qval\n",
      "     8  2677 IPR036188        max   2000   2051 0.987 0.983 0.990 4047156.5 0.00e+00    0.986630           True    -0.199003    -0.019933 0.00e+00\n",
      "     8  2677 IPR036188       mean   2000   2051 0.214 0.200 0.228  877278.5 1.00e+00    0.213866           True    -0.064228    -0.141996 1.00e+00\n",
      "     8  2677 IPR036188       topk   2000   2051 0.911 0.902 0.920 3735457.5 0.00e+00    0.910643           True     0.094369     0.147390 0.00e+00\n",
      "     8  2677 IPR036188       topq   2000   2051 0.984 0.980 0.988 4035459.5 0.00e+00    0.983779           True    -0.671316    -0.093516 0.00e+00\n",
      "\n",
      "Saved: reconcile_out_8_2677_bh/results_reconcile.csv\n",
      "Plots saved under: ./reconcile_out_8_2677_bh\n"
     ]
    }
   ],
   "source": [
    "TARGETS = [(8, 2677, \"IPR036188\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_circuits/data/protein-matching-IPR036188.fasta\")]\n",
    "\n",
    "run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=2000,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method='bh',\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out_8_2677_bh\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "798cbb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6004599630bd4b72a521d841ca2a1a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "targets:   0%|          | 0/1 [00:00<?, ?latent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01946408cca649308e2b06158a7868e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L12.1082 PF00867  pos seqs:   0%|          | 0/220 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ffd788c1de445eb902ca5a392c4785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L12.1082 PF00867  neg seqs:   0%|          | 0/223 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f5bb2fed104a6b8a9bc4c968607296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [max]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad44ed7aa2f54623a1a2c6b32072617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [topq]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77572394fc7f4f7990c5240d481988cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [mean]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ed71baa14e4b3cbdfb6629a1ff9f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L12.1082 PF00867 [topk]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary (sorted by AUC) ===\n",
      " layer  unit  domain aggregator  n_pos  n_neg   auc ci_lo ci_hi       U    p_mwu  auc_from_U  auc_matches_U  rho_len_pos  rho_len_neg     qval\n",
      "    12  1082 PF00867        max    220    223 0.998 0.995 1.000 48984.5 6.38e-74    0.998461           True     0.688171     0.063035 2.55e-73\n",
      "    12  1082 PF00867       mean    220    223 0.585 0.527 0.643 28702.5 9.79e-04    0.585049           True     0.579286     0.040839 9.79e-04\n",
      "    12  1082 PF00867       topk    220    223 0.723 0.671 0.771 35447.5 2.68e-16    0.722534           True     0.736398     0.153643 3.58e-16\n",
      "    12  1082 PF00867       topq    220    223 0.996 0.988 1.000 48848.5 3.99e-73    0.995689           True    -0.061831     0.025616 7.97e-73\n",
      "\n",
      "Saved: reconcile_out_12_1082_bonferroni/results_reconcile.csv\n",
      "Plots saved under: ./reconcile_out_12_1082_bonferroni\n"
     ]
    }
   ],
   "source": [
    "TARGETS = [(12, 1082, \"PF00867\",  \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-PF00867.fasta\")]\n",
    "\n",
    "run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=MAX_POS_SAMPLES,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method='bonferroni',\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out_12_1082_bonferroni\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de3ae553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865907a2afe343caa2cd5c34783a42fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "targets:   0%|          | 0/1 [00:00<?, ?latent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ed7ff3ffea4ebf9c4eb844f4307171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L8.488 IPR029058  pos seqs:   0%|          | 0/2937 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351434c9644c4a0eb23290e264beede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L8.488 IPR029058  neg seqs:   0%|          | 0/2957 [00:00<?, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca38b13b07f47fd863bdafb92f1f6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.488 IPR029058 [max]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb5bf44860f4954a30184c4e9ad62c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.488 IPR029058 [topq]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b38ec813144d4e97a5a8d49e466f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.488 IPR029058 [mean]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751d90e909c445458e272e9e2dc9da83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boot L8.488 IPR029058 [topk]:   0%|          | 0/3000 [00:00<?, ?boot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary (sorted by AUC) ===\n",
      " layer  unit    domain aggregator  n_pos  n_neg   auc ci_lo ci_hi         U    p_mwu  auc_from_U  auc_matches_U  rho_len_pos  rho_len_neg     qval\n",
      "     8   488 IPR029058        max   2937   2957 0.977 0.973 0.980 8483791.5 0.00e+00    0.976865           True    -0.178210     0.220185 0.00e+00\n",
      "     8   488 IPR029058       mean   2937   2957 0.641 0.627 0.656 5569878.0 4.29e-79    0.641343           True     0.138591     0.082591 4.29e-79\n",
      "     8   488 IPR029058       topk   2937   2957 0.851 0.841 0.860 7392031.5 0.00e+00    0.851155           True     0.385099     0.421938 0.00e+00\n",
      "     8   488 IPR029058       topq   2937   2957 0.971 0.967 0.975 8433572.5 0.00e+00    0.971083           True    -0.559665     0.089352 0.00e+00\n",
      "\n",
      "Saved: reconcile_out_8_488/results_reconcile.csv\n",
      "Plots saved under: ./reconcile_out_8_488\n"
     ]
    }
   ],
   "source": [
    "TARGETS = [(8, 488,  \"IPR029058\", \"/work/pi_jensen_umass_edu/jnainani_umass_edu/plm_interp/data/paper/protein-matching-IPR029058.fasta\")]\n",
    "\n",
    "run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=MAX_POS_SAMPLES,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method=MT_METHOD,\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out_8_488\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a626982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------ Runner -----------------------------------\n",
    "\n",
    "def run_experiment(\n",
    "    targets: List[Tuple[int,int,str,str]],\n",
    "    fasta_sprot: str,\n",
    "    aggregators: List[str] = AGGREGATORS,\n",
    "    neg_match: str = NEG_MATCH_STRAT,\n",
    "    n_bins: int = N_LENGTH_BINS,\n",
    "    max_seq_len: int = MAX_SEQ_LEN,\n",
    "    max_pos_samples: int = MAX_POS_SAMPLES,\n",
    "    seed: int = SEED,\n",
    "    boot_n: int = BOOT_N,\n",
    "    mt_method: str = MT_METHOD,\n",
    "    alpha: float = ALPHA,\n",
    "    alternative: str = ALTERNATIVE,\n",
    "    outdir: str = \"./reconcile_out\"\n",
    "):\n",
    "    out = pathlib.Path(outdir); out.mkdir(parents=True, exist_ok=True)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Load negative pool (Swiss-Prot / reviewed)\n",
    "    neg_pool = parse_fasta(fasta_sprot, max_len=max_seq_len)\n",
    "    if len(neg_pool) == 0:\n",
    "        raise RuntimeError(f\"No sequences loaded from {fasta_sprot}\")\n",
    "    neg_pool_ids = np.array([pid for pid,_ in neg_pool], dtype=object)\n",
    "    neg_pool_seqs = np.array([seq for _,seq in neg_pool], dtype=object)\n",
    "    neg_pool_lens = np.array([len(s) for s in neg_pool_seqs], dtype=int)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for (layer, unit, domain, pos_fasta) in targets:\n",
    "        # Load positives for this domain\n",
    "        pos_entries = parse_fasta(pos_fasta, max_len=max_seq_len)\n",
    "        if len(pos_entries) == 0:\n",
    "            print(f\"[WARN] No positives loaded from {pos_fasta}; skipping {domain}.\")\n",
    "            continue\n",
    "        if max_pos_samples is not None and len(pos_entries) > max_pos_samples:\n",
    "            pos_entries = list(rng.choice(pos_entries, size=max_pos_samples, replace=False))\n",
    "        pos_ids = np.array([pid for pid,_ in pos_entries], dtype=object)\n",
    "        pos_seqs = np.array([seq for _,seq in pos_entries], dtype=object)\n",
    "        pos_lens = np.array([len(s) for s in pos_seqs], dtype=int)\n",
    "\n",
    "        # Exclude any positives from negative pool by ID overlap\n",
    "        mask_not_pos = ~np.isin(neg_pool_ids, pos_ids)\n",
    "        neg_ids_pool = neg_pool_ids[mask_not_pos]\n",
    "        neg_seqs_pool = neg_pool_seqs[mask_not_pos]\n",
    "        neg_lens_pool = neg_pool_lens[mask_not_pos]\n",
    "\n",
    "        # Select length-matched negatives\n",
    "        if neg_match == \"length\":\n",
    "            idx = sample_length_matched_negatives(pos_lens, neg_lens_pool, rng, n_bins=n_bins)\n",
    "            if idx.size == 0:\n",
    "                # fallback: random sample equal size\n",
    "                idx = rng.choice(np.arange(len(neg_seqs_pool)), size=len(pos_seqs), replace=False)\n",
    "        else:\n",
    "            idx = rng.choice(np.arange(len(neg_seqs_pool)), size=len(pos_seqs), replace=False)\n",
    "\n",
    "        neg_ids = neg_ids_pool[idx]\n",
    "        neg_seqs = neg_seqs_pool[idx]\n",
    "        neg_lens = neg_lens_pool[idx]\n",
    "\n",
    "        # Compute token activations for all sequences (vectorized loop)\n",
    "        def per_protein_score(seq: str, agg_name: str) -> float:\n",
    "            tok = compute_token_activations(seq, layer, unit)\n",
    "            if not isinstance(tok, np.ndarray):\n",
    "                tok = np.asarray(tok)\n",
    "            func = AGG_FUNCS[agg_name]\n",
    "            return float(func(tok))\n",
    "\n",
    "        # Precompute scores per aggregator\n",
    "        for agg in aggregators:\n",
    "            pos_scores = np.array([per_protein_score(s, agg) for s in pos_seqs], dtype=np.float64)\n",
    "            neg_scores = np.array([per_protein_score(s, agg) for s in neg_seqs], dtype=np.float64)\n",
    "\n",
    "            # Build y and s\n",
    "            y = np.r_[np.ones(pos_scores.size, dtype=int), np.zeros(neg_scores.size, dtype=int)]\n",
    "            s = np.r_[pos_scores, neg_scores]\n",
    "\n",
    "            # Metrics\n",
    "            auc, (lo, hi) = stratified_boot_auc(y, s, B=boot_n, seed=seed)\n",
    "            U, p, aucU = mwu_test(pos_scores, neg_scores, alternative=alternative)\n",
    "            agree = abs(auc - aucU) < 1e-6\n",
    "\n",
    "            # Length correlations (diagnostic)\n",
    "            rho_pos, _ = spearmanr(pos_scores, pos_lens)\n",
    "            rho_neg, _ = spearmanr(neg_scores, neg_lens)\n",
    "\n",
    "            rows.append({\n",
    "                \"layer\": layer, \"unit\": unit, \"domain\": domain, \"aggregator\": agg,\n",
    "                \"n_pos\": int(pos_scores.size), \"n_neg\": int(neg_scores.size),\n",
    "                \"auc\": float(auc), \"ci_lo\": float(lo), \"ci_hi\": float(hi),\n",
    "                \"U\": float(U), \"p_mwu\": float(p), \"auc_from_U\": float(aucU), \"auc_matches_U\": bool(agree),\n",
    "                \"rho_len_pos\": float(rho_pos), \"rho_len_neg\": float(rho_neg),\n",
    "            })\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No results produced; check your inputs.\")\n",
    "        return\n",
    "\n",
    "    # Multiple testing correction\n",
    "    if MT_METHOD.lower() == \"bh\":\n",
    "        df[\"qval\"] = benjamini_hochberg(df[\"p_mwu\"].values)\n",
    "        mt_label = \"BH-FDR qval\"\n",
    "    elif MT_METHOD.lower() == \"bonferroni\":\n",
    "        m = df.shape[0]\n",
    "        df[\"qval\"] = np.minimum(1.0, df[\"p_mwu\"].values * m)\n",
    "        mt_label = \"Bonferroni-adjusted p\"\n",
    "    else:\n",
    "        df[\"qval\"] = df[\"p_mwu\"]\n",
    "        mt_label = \"p (uncorrected)\"\n",
    "\n",
    "    # Save outputs\n",
    "    out_csv = pathlib.Path(outdir) / \"results_reconcile.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # Pretty print summary\n",
    "    print(\"\\n=== Summary (sorted by AUC) ===\")\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(df.sort_values([\"aggregator\", \"auc\"], ascending=[True, False]).to_string(index=False,\n",
    "              formatters={\"auc\":\"{:.3f}\".format, \"ci_lo\":\"{:.3f}\".format, \"ci_hi\":\"{:.3f}\".format,\n",
    "                          \"p_mwu\":\"{:.2e}\".format, \"qval\":\"{:.2e}\".format}))\n",
    "\n",
    "    # Save one plot per aggregator (barh with CI)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        for agg in sorted(df[\"aggregator\"].unique()):\n",
    "            sub = df[df[\"aggregator\"] == agg].sort_values(\"auc\", ascending=True)\n",
    "            y = np.arange(sub.shape[0])\n",
    "            plt.figure(figsize=(6.0, max(2.5, 0.35*len(y))))\n",
    "            plt.barh(y, sub[\"auc\"].values, color=\"tab:blue\", edgecolor=\"black\", height=0.6)\n",
    "            xerr = np.vstack([sub[\"auc\"].values - sub[\"ci_lo\"].values,\n",
    "                              sub[\"ci_hi\"].values - sub[\"auc\"].values])\n",
    "            plt.errorbar(sub[\"auc\"].values, y, xerr=xerr, fmt=\"none\", ecolor=\"black\", capsize=3, lw=1)\n",
    "            plt.axvline(0.5, color=\"grey\", ls=\"--\", lw=1)\n",
    "            labels = [f\"L{L}.{U}\\n{D}\" for L,U,D in zip(sub[\"layer\"], sub[\"unit\"], sub[\"domain\"])]\n",
    "            plt.yticks(y, labels, fontsize=9)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.xlabel(\"AUROC (higher = more selective)\")\n",
    "            plt.title(f\"Selectivity by {agg} (CI = 95% stratified bootstrap)\\n{mt_label}: mark in CSV\", fontsize=11)\n",
    "            plt.tight_layout()\n",
    "            out_png = pathlib.Path(outdir) / f\"plot_{agg}.png\"\n",
    "            plt.savefig(out_png, dpi=200)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "    # Save JSON metadata\n",
    "    meta = {\n",
    "        \"fasta_sprot\": fasta_sprot,\n",
    "        \"targets\": TARGETS,\n",
    "        \"aggregators\": AGGREGATORS,\n",
    "        \"params\": {\n",
    "            \"TOPQ\": TOPQ, \"LSE_TAU\": LSE_TAU, \"MAX_SEQ_LEN\": MAX_SEQ_LEN,\n",
    "            \"NEG_MATCH_STRAT\": NEG_MATCH_STRAT, \"N_LENGTH_BINS\": N_LENGTH_BINS,\n",
    "            \"SEED\": SEED, \"BOOT_N\": BOOT_N, \"ALTERNATIVE\": ALTERNATIVE,\n",
    "            \"MT_METHOD\": MT_METHOD, \"ALPHA\": ALPHA,\n",
    "        },\n",
    "    }\n",
    "    with open(pathlib.Path(outdir) / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved: {out_csv}\")\n",
    "    print(f\"Plots (per aggregator) saved under: {outdir}\")\n",
    "\n",
    "# ---------------------------- Main entry ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ba7908",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGETS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfasta_sprot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFASTA_SPROT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAGGREGATORS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneg_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEG_MATCH_STRAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_LENGTH_BINS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_pos_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_POS_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboot_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBOOT_N\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmt_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMT_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43malternative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALTERNATIVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./reconcile_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(targets, fasta_sprot, aggregators, neg_match, n_bins, max_seq_len, max_pos_samples, seed, boot_n, mt_method, alpha, alternative, outdir)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agg \u001b[38;5;129;01min\u001b[39;00m aggregators:\n\u001b[1;32m     72\u001b[0m     pos_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([per_protein_score(s, agg) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pos_seqs], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 73\u001b[0m     neg_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([per_protein_score(s, agg) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m neg_seqs], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Build y and s\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[np\u001b[38;5;241m.\u001b[39mones(pos_scores\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros(neg_scores\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agg \u001b[38;5;129;01min\u001b[39;00m aggregators:\n\u001b[1;32m     72\u001b[0m     pos_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([per_protein_score(s, agg) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pos_seqs], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 73\u001b[0m     neg_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mper_protein_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m neg_seqs], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Build y and s\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[np\u001b[38;5;241m.\u001b[39mones(pos_scores\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros(neg_scores\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[7], line 64\u001b[0m, in \u001b[0;36mrun_experiment.<locals>.per_protein_score\u001b[0;34m(seq, agg_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mper_protein_score\u001b[39m(seq: \u001b[38;5;28mstr\u001b[39m, agg_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     tok \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_token_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tok, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     66\u001b[0m         tok \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tok)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mcompute_token_activations\u001b[0;34m(seq, layer_idx, unit_idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m h \u001b[38;5;241m=\u001b[39m esm_transformer\u001b[38;5;241m.\u001b[39mesm\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer[layer_idx]\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mesm_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_contacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m h\u001b[38;5;241m.\u001b[39mremove(); torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sae_layer\u001b[38;5;241m.\u001b[39mfeature_acts[:, unit_idx]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:1035\u001b[0m, in \u001b[0;36mEsmForMaskedLM.predict_contacts\u001b[0;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_contacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, attention_mask):\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_contacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:935\u001b[0m, in \u001b[0;36mEsmModel.predict_contacts\u001b[0;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_contacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, attention_mask):\n\u001b[0;32m--> 935\u001b[0m     attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mattentions\n\u001b[1;32m    936\u001b[0m     attns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(attns, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Matches the original model layout\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;66;03m# In the original model, attentions for padding tokens are completely zeroed out.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;66;03m# This makes no difference most of the time because the other tokens won't attend to them,\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# but it does for the contact prediction task, which takes attentions as input,\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# so we have to mimic that here.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:907\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    900\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    901\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    902\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    906\u001b[0m )\n\u001b[0;32m--> 907\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    920\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:612\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    602\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:502\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m ):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:436\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m ):\n\u001b[1;32m    435\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 436\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    446\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=MAX_POS_SAMPLES,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method=MT_METHOD,\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    run_experiment(\n",
    "        targets=TARGETS,\n",
    "        fasta_sprot=FASTA_SPROT,\n",
    "        aggregators=AGGREGATORS,\n",
    "        neg_match=NEG_MATCH_STRAT,\n",
    "        n_bins=N_LENGTH_BINS,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        max_pos_samples=MAX_POS_SAMPLES,\n",
    "        seed=SEED,\n",
    "        boot_n=BOOT_N,\n",
    "        mt_method=MT_METHOD,\n",
    "        alpha=ALPHA,\n",
    "        alternative=ALTERNATIVE,\n",
    "        outdir=\"./reconcile_out\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24727b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
