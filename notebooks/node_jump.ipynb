{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4434aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython\n",
      "Set autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"../plm_circuits/\")\n",
    "from helpers.utils import (\n",
    "    load_esm,\n",
    "    load_sae_prot,\n",
    "    mask_flanks_segment,\n",
    "    cleanup_cuda,\n",
    "    patching_metric,\n",
    ")\n",
    "from attribution import integrated_gradients_sae\n",
    "from hook_manager import SAEHookProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "def get_unmask_indices(seq_len: int, start: int, end: int, flank_len: int, is_left: bool):\n",
    "    \"\"\"Return the indices to unmask on the left or right flank.\"\"\"\n",
    "    if is_left:\n",
    "        left_start = max(0, start - flank_len)\n",
    "        return list(range(left_start, start))\n",
    "    else:\n",
    "        right_end = min(seq_len, end + flank_len)\n",
    "        return list(range(end, right_end))\n",
    "\n",
    "def get_masked_sequence(seq, ss1_start, ss1_end, ss2_start, ss2_end, flank_len, seq_len):\n",
    "    \"\"\"Mask a sequence with flanks of given length around ss1 and ss2.\"\"\"\n",
    "    unmask_left_idxs = get_unmask_indices(seq_len, ss1_start, ss1_end, flank_len, is_left=True)\n",
    "    unmask_right_idxs = get_unmask_indices(seq_len, ss1_start, ss2_end, flank_len, is_left=False)\n",
    "    return mask_flanks_segment(seq, ss1_start, ss1_end, ss2_start, ss2_end, unmask_left_idxs, unmask_right_idxs)\n",
    "\n",
    "def tokenize_sequence(batch_converter, seq, padding_idx, device):\n",
    "    \"\"\"Convert sequence to token tensor and corresponding mask.\"\"\"\n",
    "    _, _, tokens = batch_converter([(1, seq)])\n",
    "    tokens = tokens.to(device)\n",
    "    mask = (tokens != padding_idx).to(device)\n",
    "    return tokens, mask\n",
    "\n",
    "def run_sae_hooked_prediction(\n",
    "    model, sae_model, tokens, mask, layer_idx, patching_metric_fn\n",
    "):\n",
    "    \"\"\"Run the model with a hooked SAE layer and return activations, error, contact prediction, and recovery metric.\"\"\"\n",
    "    hook = SAEHookProt(\n",
    "        sae=sae_model,\n",
    "        mask_BL=mask,\n",
    "        cache_latents=True,\n",
    "        layer_is_lm=False,\n",
    "        calc_error=True,\n",
    "        use_error=True,\n",
    "    )\n",
    "    handle = model.esm.encoder.layer[layer_idx].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        contact_LL = model.predict_contacts(tokens, mask)[0]\n",
    "    cleanup_cuda()\n",
    "    handle.remove()\n",
    "\n",
    "    return sae_model.feature_acts, sae_model.error_term, contact_LL, patching_metric_fn(contact_LL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ecc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "esm_transformer, batch_converter, esm2_alphabet = load_esm(33, device=device)\n",
    "\n",
    "main_layers = [4, 8, 12, 16, 20, 24, 28]\n",
    "saes = []\n",
    "for layer in main_layers:\n",
    "    sae_model = load_sae_prot(ESM_DIM=1280, SAE_DIM=4096, LAYER=layer, device=device)\n",
    "    saes.append(sae_model)\n",
    "\n",
    "layer_2_saelayer = {layer: layer_idx  for layer_idx, layer in enumerate(main_layers)}\n",
    "\n",
    "with open('../data/full_seq_dict.json', \"r\") as json_file:\n",
    "    seq_dict = json.load(json_file)\n",
    "\n",
    "sse_dict = {\"2B61A\": [[182, 316]],\"1PVGA\": [[101, 202]]}\n",
    "fl_dict = {\"2B61A\": [44, 43], \"1PVGA\": [65, 63]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ca7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"2B61A\"\n",
    "seq = seq_dict[protein]\n",
    "L = len(seq)\n",
    "\n",
    "position = sse_dict[protein][0]\n",
    "ss1_start, ss1_end = position[0] - 5, position[0] + 6\n",
    "ss2_start, ss2_end = position[1] - 5, position[1] + 6\n",
    "\n",
    "# Full tokens\n",
    "full_seq_L = [(1, seq)]\n",
    "_, _, batch_tokens_BL = batch_converter(full_seq_L)\n",
    "batch_tokens_BL = batch_tokens_BL.to(device)\n",
    "batch_mask_BL = (batch_tokens_BL != esm2_alphabet.padding_idx).to(device)\n",
    "\n",
    "# Clean input\n",
    "clean_fl = fl_dict[protein][0]\n",
    "clean_seq = get_masked_sequence(seq, ss1_start, ss1_end, ss2_start, ss2_end, clean_fl, L)\n",
    "clean_batch_tokens_BL, clean_batch_mask_BL = tokenize_sequence(batch_converter, clean_seq, esm2_alphabet.padding_idx, device)\n",
    "\n",
    "# Corrupted input\n",
    "corr_fl = fl_dict[protein][1]\n",
    "corr_seq = get_masked_sequence(seq, ss1_start, ss1_end, ss2_start, ss2_end, corr_fl, L)\n",
    "corr_batch_tokens_BL, corr_batch_mask_BL = tokenize_sequence(batch_converter, corr_seq, esm2_alphabet.padding_idx, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd75f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4: Clean contact recovery: 1.3437, Corr contact recovery: 13.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:00<00:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 8: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:01<00:01,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:01<00:01,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:01<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:02<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28: Clean contact recovery: 1.3437, Corr contact recovery: 13.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# all_effects_sae_ALS = []\n",
    "# all_effects_err_ABLF = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_seq_contact_LL = esm_transformer.predict_contacts(batch_tokens_BL, batch_mask_BL)[0]\n",
    "cleanup_cuda()\n",
    "\n",
    "_patching_metric = partial(\n",
    "    patching_metric,\n",
    "    full_seq_contact_LL,\n",
    "    ss1_start=ss1_start,\n",
    "    ss1_end=ss1_end,\n",
    "    ss2_start=ss2_start,\n",
    "    ss2_end=ss2_end,\n",
    ")\n",
    "\n",
    "for layer_idx in tqdm(main_layers):\n",
    "    sae_model = saes[layer_2_saelayer[layer_idx]]\n",
    "\n",
    "    clean_cache_LS, clean_err_BLF, clean_contact_LL, clean_recovery = run_sae_hooked_prediction(\n",
    "        esm_transformer, sae_model, clean_batch_tokens_BL, clean_batch_mask_BL, layer_idx, _patching_metric\n",
    "    )\n",
    "\n",
    "    corr_cache_LS, corr_err_BLF, corr_contact_LL, corr_recovery = run_sae_hooked_prediction(\n",
    "        esm_transformer, sae_model, corr_batch_tokens_BL, corr_batch_mask_BL, layer_idx, _patching_metric\n",
    "    )\n",
    "\n",
    "    print(f\"Layer {layer_idx}: Clean contact recovery: {clean_recovery:.4f}, Corr contact recovery: {corr_recovery:.4f}\")\n",
    "\n",
    "    # effect_sae_LS, effect_err_BLF = integrated_gradients_sae(\n",
    "    #     esm_transformer,\n",
    "    #     sae_model,\n",
    "    #     _patching_metric,\n",
    "    #     clean_cache_LS,\n",
    "    #     corr_cache_LS,\n",
    "    #     clean_err_BLF,\n",
    "    #     corr_err_BLF,\n",
    "    #     batch_tokens=clean_batch_tokens_BL,\n",
    "    #     batch_mask=clean_batch_mask_BL,\n",
    "    #     hook_layer=layer_idx,\n",
    "    # )\n",
    "\n",
    "    # all_effects_sae_ALS.append(effect_sae_LS)\n",
    "    # all_effects_err_ABLF.append(effect_err_BLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875cbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1c8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097f8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
